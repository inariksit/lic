\section{Introduction to CG}
 
\paragraph{This used to be my course paper for Grammar Formalisms.}

(TODO: should I write here something in depth about e.g. set operations in target/conditions?)

Constraint Grammar (CG) is a formalism for 
disambiguating morphologically analysed text. 
It was first introduced by
\cite{karlsson1995constraint}, and has been used for many tasks in
computational linguistics, such as POS tagging, surface syntax and
machine translation \cite{bick2011}. It disambiguates output by
morphological analyser by using
constraint rules which can select or remove a potential analysis
(called `reading') for a
target word, depending on the context words around it. Together these rules disambiguate the whole text.

In the example below, I show possible analyses for the sentence ``the bear
sleeps''. 
To introduce the terminology: each \emph{word form} appears surrounded by "< >".
The analyses for each word form contain one lemma, and a list of morphological tags. A list of lemma and tags is called \emph{reading}, and a word form together with its readings is called a \emph{cohort}. A cohort is ambiguous, if it contains more than one reading.

\begin{center}
\begin{verbatim}
"<the>"
        "the" det def
"<bear>"
        "bear" noun sg
        "bear" verb pres
        "bear" verb inf
"<sleeps>"
        "sleep" noun pl
        "sleep" verb pres p3 sg
\end{verbatim}
\end{center}


\noindent We can disambiguate this sentence with two rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item \texttt{REMOVE verb IF (-1 det)}
  `Remove verb after determiner'
\item  \texttt{REMOVE noun IF (-1 noun)}
  `Remove noun after noun'
\end{enumerate}

\noindent Rule 1 matches the word \emph{bear}: it is tagged as verb and is
preceded by a determiner. The rule removes both verb readings from
\emph{bear}, leaving it with an unambiguous analysis \texttt{noun sg}.
Rule 2 is applied to the word \emph{sleeps}, and it removes the noun
reading. The finished analysis is shown below:


\begin{verbatim}
"<the>"
        "the" det def
"<bear>"
        "bear" noun sg
"<sleeps>"
        "sleep" verb pres p3 sg
\end{verbatim}

It is also possible to add syntactic tags and dependency structure within CG. 
After disambiguating \emph{bear} by part of speech (noun), it
remains to disambiguate whether it is a subject, object, adverbial or
any other possible syntactic role.
This can be done in several ways. One option is to get both
syntactic and morphological analyses from the initial parser---the analysis
for \emph{bear} would look like the following:

\begin{verbatim}
"<bear>"
        "bear" noun sg           @Subj
        "bear" noun sg           @Obj
        "bear" verb pres         @Pred
        "bear" verb inf 
\end{verbatim}

\noindent Morphological disambiguation rules would
resolve \emph{bear} into a noun, and syntactic disambiguation rules
would be applied later, to resolve \emph{bear}-noun into subject or object.

Alternatively, one can have the initial parser return only
morphological tags, and use CG rules that map a syntactic tag to a
reading (or a reading to a cohort).
The rules to add work similarly to the rules to remove and select:
if a target matches the context, then the new tag is added.
These rules are usually run after the morphological disambiguation is
finished.
The following rules could be applied to the example sentence. The
letter `C' after the position means \emph{careful} context: e.g. 
\texttt{(-1C noun)} requires the previous word to be unambiguously
noun. Such rule would not fire if the previous word has any other
reading in addition to the noun reading.

\begin{enumerate}
\itemsep1pt\parskip0pt\parsep0pt
\item[] \texttt{MAP @Pred IF (0C verb) (-1C noun)}
\item[] \texttt{MAP @Subj IF (0C noun) (1C verb)}
\end{enumerate}

\noindent With both of the strategies, we would end up with the
following output:

\begin{verbatim}
"<the>"
        "the" det def            
"<bear>"
        "bear" noun sg           @Subj 
"<sleeps>"
        "sleep" verb pres p3 sg  @Pred

\end{verbatim}

The syntactic tags can also indicate some dependency structure. For
instance, \emph{the} could get a tag such as
\texttt{@DN\textgreater}, to indicate that it is a determiner whose
head is to the right. A phrase such as \emph{the little dog} could get
tagged as  \emph{the}\texttt{DN>} \emph{little}\texttt{AN>}
\emph{bear}\texttt{@Subj}.
 Using \texttt{\textless{}tag} and \texttt{tag\textgreater} can identify
chunks, as long as they are not disconnected.

With the introduction of CG-3, it is possible to add full-fledged dependency
relations: number the words in the sentence and for each set a parent,
such as \texttt{"<the>" "the" det def  \textbf{IND=1 PARENT=2}}. 
However, these features are recent and not used in many of the grammars written 
in the community. In this introduction, we will illustrate the examples with the 
most basic operations, that is, disambiguating morphological tags.
The syntactic operations are not fundamentally
different from morphological: the rules describe an \emph{operation}
performed on a \emph{target}, conditional on a \emph{context}.




\section{Properties of Constraint Grammar}\label{properties}

\cite{karlsson1995constraint} lists 24 design principles and describes
related work at the time of writing.
Here we summarise a set of main features, and relate CG to the developments in grammar formalism since the initial CG description.

All theories and implementations of CG  a \emph{reductionist}
system, designed primarily for analysis, not generation.
Its task is to parse sentences that are given, not to describe a language
as a collection of ``all and only the grammatical sentences''.

The syntax is decidedly \emph{shallow}: the rules do not aim to
describe all aspects of an abstract phenomenon such as noun phrase, 
but rather bits and pieces with concrete conditions.
The rules are self-contained and mutually independent---this makes it 
easy to add exceptions, and exceptions to exceptions, without 
changing the more general rules.

There are different takes on how \emph{deterministic} the rules are.
The current state-of-the-art CG parser VISL CG-3 executes the rules strictly based on the order of appearance, but there
are other implementations that apply their own heuristics or remove the
ordering in total, such as in finite-state or logic-based implementations. 

In addition, we will discuss the \emph{expressivity} of CG---while it is not a generative grammar and cannot be placed in the Chomsky hierarchy, we aim to provide parallels in what kind of output it can produce.

\subsection{Reductionist vs.~licencing (or Analysis vs. Generation???)}\label{reductionist-vs.licencing}

CG is a reductionist system: it starts from a set of alternative
analyses, and eliminates the impossible or improbable ones using
constraint rules. The remaining analyses are assumed to be correct; that
is, everything that is not explicitly eliminated, is allowed. This can
be contrasted with a licencing system, where constructions must
be explicitly allowed, otherwise they are illegal. An empty
reductionist grammar will accept any string, whereas an empty
licencing grammar will accept no string.

\paragraph{From Fred's 1995 book}
\begin{quote}
Notice, in passing, that the overall reductionistic set-up gives an
interesting psycholinguistic prediction. In terms of processing load,
less processing is required under Constraint Grammar to turn out an
ambiguous and therefore unclear analysis than to turn out a
disambiguated and therefore structurally clear analysis. In our
opinion, this seems to be a reasonable psycholinguistic hypothesis a priori.
Mental effort is needed for achieving clarity, precision, and maximal
information. Less efforts imply (retention of) unclarity and
ambiguity, i.e. information decrease. In several types of parsers, 
rule applications create rather than discard ambiguities: the more
processing, the less unambiguous information.
\end{quote}

According to the initial specification, it is a grammar for analysis: \cite{karlsson1995constraint} does not see it fit for generation:

\begin{quote}
It would be optimal if the formalism were general and abstract enough for it, or rather for grammars written in the framework of it, to be used both for sentence analysis and sentence generation. 
Constraint Grammar takes no explicit stand on this issue. In practice, however, constraints are geared towards parsing, and Constraint Grammars are analysis grammars. It remains to be demonstrated that full-scale syntax can be done by one and the same reversible grammar.
\end{quote}


Two decades later, \cite{bick2015} describe CG as ``a
declarative whole of contextual possibilities and impossibilities for
a language or genre'', which is nevertheless implemented in a
low-level way: selecting and removing readings from individual words,
without explicit connection between the rules. Bick and Didriksen
argue that as a side effect, the rules actually manage to describe language.

We return to the questions of generation and declarativity in section~\ref{sec:expressivity}. 
In chapter~\ref{chapterCGana}, we revisit these questions in the context of CG analysis.

 
\paragraph{No enforcement of grammaticality} 

\cite{karlsson1995constraint} states explicitly:

\begin{quote}
The foremost task of a parsing-oriented grammar is rather to aid in parsing every input than to define the notion ”grammatically correct sentence”, or to describe ”all and only the grammatical sentences”.
\end{quote}


CG does not define sequences as \emph{grammatically correct}.
Beside the lack of long-distance dependencies, there is
no mandatory enforcement of even local well-formedness, such as gender agreement.
However, this is often helpful for practical purposes, because it adds robustness.
Let us illustrate with an example in Swedish.

\begin{verbatim}
"<en>"
        "en" det indef utr sg
        "en" noun utr sg 
"<bord>"
        "bord" noun neutr sg
\end{verbatim}

The first word, \emph{en}, is ambiguous between the indefinite determiner for
\emph{utrum} gender  (masculine and feminine collapsed into one), or a noun (`juniper').
The second word, \emph{bord}, is a neuter noun (`table')---the writer has
most likely meant to write ``a table'' instead of ``juniper table'', but has mistaken the gender of `table'. The correct form, which also would not be ambiguous, is ``ett bord''.

CG allows the rules to be as fine-grained or broad as we want: 
\texttt{SELECT det IF (1 noun) (0 noun)} would match any determiner and any noun, 
and succesfully disambiguate \emph{en} in this case. 
We can also enforce grammaticality by writing \texttt{SELECT det utr sg IF (1 noun utr sg)}%\footnote{the counterpart neuter determiner is not ambiguous with noun, so \texttt{SELECT det neutr sg IF (1 noun neutr sg) (0 noun)} would be redundant. Though it can be ambiguous with the numeral `1'. So actually maybe it's good anyway.}
. In that case, nothing in the example phrase matches, and it is left ambiguous.


%On the other hand, we can introduce such fine-graind rules in order to pick the right analysis, for instance, if a certain modifier is underspecified. If we analyse Spanish text \emph{la casa grande}, where \emph{grande} is underspecified for gender, then having a rule that picks the feminine adjective after a feminine article and feminine noun makes our analysis of \emph{grande} more accurate. 
% a certain adjective is underspecified for gender but its head noun is specified, then we can pick the right gender for the adjective.

The previous example illustrates that the notions of \emph{accept} and \emph{reject} are not clear: if agreement was enforced
by enumerating only legal combinations, the grammar would just refuse to disambiguate
an ungrammatical sequence and leave all tags intact---in that case,
its performance would not differ from a grammar that simply does not have many rules.
If we define \emph{grammatical sequences} to be sequences of POS tags,
then the question is somewhat easier to answer: \texttt{\textless{}det\textgreater{}\textless{}utr\textgreater{} \textless{}noun\textgreater{}\textless{}neutr\textgreater{}} is ungrammatical, and CG doesn't raise an alarm that there is something wrong. However, the alternative \texttt{\textless{}noun\textgreater{}\textless{}utr\textgreater{} \textless{}noun\textgreater{}\textless{}neutr\textgreater{}} might be in that particular context even ``more wrong''; same goes for the option with no disambiguation at all.
As \citename{karlsson1995constraint} states, CG is not designed for defining grammaticality, but rather providing analyses for real-life texts.

% Of course, the latter point can be made of any reductionist system, not only CG.

\subsection{Shallow syntax}\label{shallow-syntax}

CG can be described as shallow for various reasons. The rules usually
operate on a local context of a few words. 

More importantly, CG analysis is not aimed to produce trees. (IS THIS TOO MUCH OF REPETITION OF PREVIOUS?)

Whereas a generative grammar could fail to produce any analysis, if even one of the words is unknown or misspelt, CG would, at worst, just leave that part ambiguous, and do as good a job it can elsewhere in the sentence.




\paragraph{Flat structure} 

The rules do not produce any hierarchical structure.


Syntactic labels may be added, but no invisible structure is postulated.
The syntax of CG is dependency syntax.

Syntactic labels are functionally same as morphological: \texttt{@SUBJ} is just one more tag in a reading.


The rules do not usually handle 
long-distance dependencies; they operate on a unit of a few words, and
don't abstract away from surface features such as word order.
You need to define rules separately for stuff like ``select noun if -1
determiner'' and ``select noun if -2 determiner -1 adjective''.

More importantly, CG doesn't introduce a full tree structure to the
whole sentence. Some local subtrees can be detected, but mostly
for the purposes of aiding the disambiguation.
(TODO: should I talk about dependency trees in CG-3?)



\paragraph{Description by elimination} 


\cite{karlsson1995constraint}
\begin{quote}
For example, ordinary phrase structure rules for English NPs would state that a determiner is followed by certain optional premodifiers plus an obligatory noun. A consequence of this is that a determiner cannot be followed by a finite or infinitive verb form, without an intervening noun. In Constraint Grammar, a constraint could be postulated to this effect, discarding the verbal readings of the word-form bank in expressions like the bank, the old bank.
\end{quote}



CG has a more lightweight task than e.g.~a phrase structure grammar: a
successful disambiguation can depend on just a local context, in a
window of 1-3 words left or right. To demonstrate the the difference,
let us take the same the example sentence \emph{the bear sleeps}, and a
context-free grammar with the following productions:


\begin{verbatim}
S   -> NP VP
VP  -> V NP
NP  -> Det N
Det -> "the"
N   -> "bear" | "sleeps"
V   -> "bear" | "sleeps"
\end{verbatim}

There is no way to reach \texttt{S} if \emph{bear} is analysed as
\texttt{V} and \emph{sleeps} as \texttt{N}; thus we get the individual
words disambiguated thanks to the parser, which introduces a structure
to the whole sentence. If one of the words was out of vocabulary, say
\emph{the bear warbles}, there would be no analysis for \emph{the} or
\emph{bear} either. In CG, an unrecognised verb would be no problem at
all in disambiguating the previous words.

\paragraph{Low abstraction}
Contrasting with phrase-structure or dependency grammars, the rules in
CG are often on a lower level of abstraction. 
The rule that tells to remove verb reading after a determiner
doesn't tell us about the structure of noun phrase; we need a second
rule to remove a verb after a determiner and an adjective (``the happy
bear sleeps''), and a third rule to remove verb after a possessive
pronoun (``my bear sleeps''). A phenomenon that is expressed with one
big rule in a deep formalism such as HPSG or categorial grammar, is
split into many low-level rules.

\paragraph{Mutual independence}
On the other hand, these features can provide
flexibility that is hard to mimic by a deeper formalism.
For instance, rules can target individual words
% (select \texttt{noun} if the word form is \emph{bear})
or other properties that are not generalisable to a whole word class,
such as verbs that express cognitive processes.
Introducing a subset of verbs, even if they are used only in one rule,
is very cheap and doesn't create a complicated inheritance hierarchy
of different verb types.
We can introduce a similar granularity in other systems, for instance,
a grammar in Grammatical Framework, which would include the functions
\texttt{GenVP} for whichever verbs and \texttt{CogVP} for cognitive verbs:


\begin{verbatim}
GenVP : V2    -> NP -> VP ;
CogVP : CogV2 -> NP -> VP ;
\end{verbatim}

In order to parse as much text as without the subcategorisation, the
grammar writer would have to add \texttt{CogV*} counterparts all
functions that exist for \texttt{V*}, which duplicates code; or add
coercion functions from  \texttt{CogV*} to \texttt{V*}, which increases ambiguity.
In case of multiple parses, likely the one with more information
would be preferred, but the CG solution gives more flexibility. If a
rule which matches \texttt{CogV} is introduced before (more on
ordering in the following section) a rule which matches \texttt{V}, it
is applied before, and vice versa. This allows for very fine control,
for example combining the semantic properties of the verb and the
animacy of the arguments.

\paragraph{Theoretical principles after all these practical examples} This lack of deep structure is intentional in the design of CG.
\cite{karlsson1995constraint} justifies the choice with a number of
theoretical principles: that language is open-ended and grammars are
bound to leak, and that necessary information for the syntactic analysis
comes from the morphology, hence morphology is ``the cornerstone of
syntax''. In a CG, one has access to all levels at the same time,
ranging from morphology to syntax or dependency labels, or even
semantics, if one wants to introduce semantic roles in the
tagset. One can have a rule such as \texttt{REMOVE ... IF (+1 "bear" <noun>
  @Object §Patient)}, followed by a second rule which only considers the POS of the
context word.
%  From an engineering point of view, even an incomplete CG can
% already disambiguate a lot, whereas an incomplete phrase structure
% grammar is nearly useless.

\subsection{Ordering of the rules}\label{ordering}

The previous properties of Constraint Grammar formalism and rules were specified in \cite{karlsson1995constraint}, and retained in further implementations. 
However, in the two decades following the initial specification, 
several independent implementations have experimented with different ordering
schemes. In the present section, we describe the different parameters of ordering and execution: \emph{strict vs. heuristic}, and \emph{sequential vs. parallel}.
% Even Karlsson's original specifications differ as for the \emph{ordering} of the rules:
% \cite{karlsson1990cgp} describes the CGP as executing the rules strictly in the order in which they appear, but \cite{karlsson1995constraint} states on page 17:

% \begin{quote} 
% Each single statement is true when examined in isolation, either absolutely or with some degree of certainty, depending upon how careful the grammar writer has been. Furthermore, disregarding morphosyntactic mappings, the constraints are \emph{unordered}.
% \end{quote}

% The execution of the rules depends on the following parameters: strict vs. heuristic ordering, and sequential vs. parallel execution. These parameters may combine freely: strict and sequential, strict and parallel, heuristic and sequential, or heuristic and parallel. 
Throughout the section, we will apply the rules to the following ambiguous passage:

\begin{verbatim}
"<what>"
        "what" determiner
        "what" pronoun                                        
 "<question>"
        "question" noun
        "question" verb
 "<is>"
        "be" verb
 "<this>"
        "this" determiner
\end{verbatim}

\paragraph{Strict vs. heuristic}

The first aspect concerns the ordering of the rules in the file. 
An implementation with \emph{strict order} applies each rule in the order in 
which they appear in the file. If a grammar contains the rules ``remove verb after determiner'' and ``remove noun after pronoun'' in the given order, the rule that removes the verb reading in \emph{question} will be applied first. After it has finished, there are no verb readings available anymore for the second rule to fire.
					
How do we know which rule is the right one? There can be many rules that fit the context, but we choose the one that just happens to appear first in the rule file. 
A common design pattern is to place rules with narrow set of conditions first; 
only if they do not apply, then try a rule with less conditions. For a similar effect,
a \emph{careful mode} may be used: ``remove verb after \emph{unambiguous} determiner'' 
would not fire on the first round, but it would wait for other rules to clarify the
status of \emph{what}.

% More careful rules are usually placed first, followed by stronger rules---see the pattern below.

% \begin{verbatim}					
% SELECT <rare analysis> IF <rare condition> ;
% ...
% REMOVE <rare analysis> ;
% \end{verbatim}
% If a rare condition is met, the rare analysis is selected, and since it will be the only analysis, it will be protected from the remove rule. If the rare condition is not met, the rare analysis is removed.
					
An alternative solution to a strict order is to use a \emph{heuristic order}: when disambiguating a particular word, find the rule that has the longest and most detailed match. Now, assume that there is a rule with a longer context, such as ``select noun if -1 is determiner and +1 is verb'', even if this rule appears last in the file, it would be preferred to the shorter rules, because it is a more exact match.
					
Both methods have their strengths and weaknesses. A strict order is more predictable, but it also means that the grammar writers need to pay more thought to rule interaction. A heuristic order frees the grammar writer from finding an optimal order, but it can give unexpected results, which are harder to debug.
As for major CG implementations, \cite{karlsson1990cgp} and \cite{vislcg3} follow the strict scheme, whereas \cite{tapanainen1996} is heuristic.


\paragraph{Sequential vs. parallel}

The second aspect concerns the way that sentence is processed.
%The input sentence can be processed in sequential or parallel manner.
In \emph{sequential execution}, the rules are applied to one word at a time, starting from the beginning of the sentence. The sentence is updated after each application. If the word \emph{what} gets successfully disambiguated as a pronoun, then the word \emph{question} will not match the rule ``remove verb after determiner''.


In contrast, a \emph{parallel execution} scheme disambiguates all the words at the same 
time, using their initial, ambiguous context. Both ``remove verb after determiner'' and 
``remove noun after pronoun'' will be applied to \emph{question}, because the original 
input from the morphological analyser contains both determiner and pronoun as the 
preceding word. Then, the final decision depends on any another rule that resolves \emph{what}.

To give a minimal example, parallel and sequential would disambiguate

Här är ett exempel. Antag att vi har tre ord, alla kan vara noun eller verb:

  <"w1">
       "w1" noun
       "w1" verb
  <"w2">
       "w2" noun
       "w2" verb
  <"w3">: noun, verb

Och så har vi den enda regeln:

  REMOVE verb IF (-1 verb)

Which execution scheme is better? For most purposes, a sequential execution is preferred.  Sequential execution–--together with strict or heuristic rule order---makes the rule take action immediately, and updates the context for the following words. 
If we know the correct part-of-speech for \emph{what}, then it is easier to disambiguate \emph{question}. 
In contrast, parallel execution can provide multiple solutions: 

%Parallel execution may have uses in more marginal cases: for instance, if we have a small rule set and we want to find out if the rules are consistent with each other.

Perhaps unsurprisingly, most of the existing CG implementations are sequential.
A notable exception is the system proposed in \cite{koskenniemi90}, known by the names Finite-State Intersection Grammar (FSIG) or Parallel Constraint Grammar (PCG). As per \cite{koskenniemi97}, we use the latter term. In addition, a variant of the CG engine in \cite{listenmaa_claessen2015} is parallel. 



\paragraph{Note on termination} 
Running the grammar multiple times is desirable: especially the rules
with a careful context may need other rules to run first and
disambiguate some items.
With the most basic operations, SELECT and REMOVE, we can ensure that
applying a grammar to a text will finish. The text might not be fully
disambiguated, but if there is a point where running the rules again
doesn't make a change, then the execution is stopped.

If we allow the addition of arbitrary tags to readings, or arbitrary
readings to cohorts, we cannot guarantee that running the grammar will
terminate. It is possible to add the same reading over and over again,
in which case every time we run the grammar, the argument is changed.
The syntactic tags used by MAP rules have an additional property,
that one reading can have at most one \texttt{@tag}, and if a reading has one
already, it is substituted with the old one. One can end up in a loop
substituting \texttt{@A} with \texttt{@B}.
However, specific implementations can freely add heuristics to stop
this kind of behaviour from happening.

% The constraint rules are executed in sequence, and the consequences of
% previous rule applications affect the coming rules. In addition, there
% is a default rule, which overrides all other rules: the last reading of
% a word must not be removed.
% 
% If a grammar contains the rules ``select \texttt{noun} after
% \texttt{det}'' and ``select \texttt{verb} after \texttt{det}'',
% whichever occurs first will be applied; after it has selected the noun
% reading and removed the rest, there is no verb reading available anymore
% for the second rule to fire.
% 
% More careful rules are usually placed first, followed by stronger
% rules---see the pattern below. If a rare condition is met, the rare
% analysis is selected, and since it will be the only analysis, it will be
% protected from the remove rule. If the rare condition is not met, the
% rare analysis is removed.
% 
% 
% \begin{verbatim}
% SELECT <rare analysis> IF <rare condition> ;
% ...
% REMOVE <rare analysis> ;
% \end{verbatim}
% 
\paragraph{Comparison to FSIG} 
Another example of a reductionist and shallow-syntax formalism is
Finite-State Intersection Grammar (FSIG) \citep{koskenniemi90}. In the
formalism, the tagged sequences of words, as well as rules, are modelled
as finite state automata: for instance, there are two transitions from
the state \emph{what} to the state \emph{question} in the automaton
representing \emph{what question is this}. This automaton is intersected with
a rule automaton, and each path through the resulting automaton
represents a possible analysis for the whole sequence.
 
A parallel and unordered rule set in CG corresponds to FSIG.
\cite{koskenniemi90} explicitly says that an FSIG grammar must be
consistent:
 
 \begin{quote}
 Each constraint simply adds something to the discriminating power of the
 whole grammar. No constraint rule may ever forbid something that would
 later on be accepted as an exception. This, maybe, puts more strain for
 the grammar writer but gives us better hope of understanding the grammar
 we write.
 \end{quote}


In FSIG, the path that consists of choosing the \texttt{det} arc
followed by \texttt{verb} arc is ruled out, but other interpretations
are still valid: \texttt{det+noun}, \texttt{pron+noun} and
\texttt{pron+verb}. In contrast, applying a rule in CG removes an
analysis permanently; if the rule removes the verb reading in
\emph{question}, then we cannot retrieve the \texttt{pron+verb}
possibility. Therefore, it is important to consider the order of the
rules. It is possible to operate with \emph{cautious context}, where the
rule only takes action if the context is unambiguous. For this case, the
``remove \texttt{verb} after \texttt{det}'' rule would not fire before
the word \emph{what} is fully disambiguated as a determiner.


\subsection{Expressivity}
\label{sec:expressivity}
As we have learnt, CG is not a generative grammar---it needs a list of alternatives to start removing some of them.
But can we emulate some kind of behaviour that would put it into the Chomsky hierarchy?

We can assume that an input sentence is a finite sequence, where every cohort is maximally ambiguous, ie. it contains all possible readings in $\Sigma$.


%The implementation in \cite{listenmaa_claessen2015} is not quite CG, not quite FSIG.

\section{Summary}

In this chapter, I've presented an introduction to Constraint Grammar,
highlighting some key properties and contrasting it to other grammar
formalisms.
CG is relatively young, popularised 20 years ago, and throughout its
history, it has been in favour of linguists more than computer
scientists, which might explain why its formal background has not been
studied extensively---notable exceptions being \cite{lager98} and \cite{lager_nivre01}.
 
Is CG a formalism? In order to be a grammar formalism, a candidate
framework should discriminate between grammatical and ungrammatical
sequences, and provide a structure to the grammatical sequences.
Let us start from the latter criterion. Clearly a successful run of a
high-quality rule set leaves us with more structure than we started
with. In the beginning, we know that \emph{bear} could be a verb
(infinitive or present tense, any number or person except 3rd
singular) or a noun (subject, object, predicative, ...), and in the end, we
will know the right analysis for it, given that the rule set is
good. The requirement of a good rule set doesn't affect CG's status as
a formalism or not; one could as well write a terrible context-free
grammar that doesn't correspond to natural language at all.
If POS tags are not enough to consider as a structure,
CG can also be used to manipulate syntactic analyses and even to add
dependency relations, using exactly the same mechanisms (``set parent to +1 if the word at 0 is \emph{bear} and -1 is a determiner'').

CG seems to even have some interest as a tool for psycholinguistics research:
there is a Russian being developed within the research group
 \href{https://uit.no/forskning/forskningsgrupper/gruppe?p_document_id=344365}{CLEAR
} (Cognitive Linguistics: Empirical Approaches to Russian); accessing
all levels from morphology to semantics at the same time is particularly attractive to cognitive linguists (personal communication).
As for its descriptive power, we can again compare CG to CFG. If
badly constructed, both CG and CFG can analyse almost everything,
making it useless for language description. If we accept that a
possibility to construct a bad grammar doesn't revoke CFG's status as
a formalism, it shouldn't do so for CG either. One can always compare
the relative ease of constructing a good grammar between formalisms,
but that is another topic already.

As a bottom line, CG provides a lightweight framework where already a
moderate amount of rules can be useful (e.g. \cite{lene_trond2011} with 115
rules), although high quality requires in general thousands of rules.
Like any formalism, CG itself leaves the users with much freedom; a
single grammar can be anything from completely ungrammatical rules to near-perfect description of language.

% For example \cite{bick2015} mentions a ``mature morphosyntactic core
% grammar'' which has 6000 rules. Assuming this is a typical number of
% rules, one could compare the efforts of writing those rules to writing
% a grammar of similar coverage and precision in another formalis


% This lack of structure is intentional in the design of CG;
% \cite{karlsson1995constraint} cites a number of theoretical principles,
% including

% \begin{quote}
% Language is an open-ended system where there is no strict demarcation
% line between grammatical and ungrammatical sentences. Therefore all
% grammars are bound to leak.
% \end{quote}

% \begin{quote}
% The cornerstone of syntax is morphology {[}--{]}. Syntactic rules are
% generalisations telling a) how word-forms occur in particular word order
% configurations, and b) what natural classes, ''syntactic functions'',
% can be isolated and inferred in such configurations.
% \end{quote}

% The first point also justifies a reductionist approach in favour of
% licencing; the second highlights that information comes from the
% morphological analyses