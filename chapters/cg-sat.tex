\def\la{\text{\em la}}
\def\casa{\text{\em casa}}
\def\grande{\text{\em grande}}

\def\det{{\text{\sc Det}}}
\def\prn{{\text{\sc  Prn}}}
\def\n{{\text{\sc N}}}
\def\v{{\text{\sc V}}}
\def\adj{{\text{\sc Adj}}}

\def\laDet{\la_\det}
\def\laPrn{\la_\prn}
\def\casaN{\casa_\n}
\def\casaV{\casa_\v}
\def\grandeAdj{\grande_\adj}

\def\t#1{\texttt{#1}}
\def\ob#1{\overbrace{ #1 \rule{0pt}{2ex}}}
\def\cgrule#1{{\ttfamily #1}}

\def\defRule{``do not remove the last reading''}

\def\ventero{\emph{El ventero, que, como está dicho, era un poco socarrón y ya tenía algunos
barruntos de la falta de juicio de su huésped, acabó de creerlo cuando
acabó de oírle semejantes razones, y, por tener qué reír aquella noche,
determinó de seguirle el humor; y así, le dijo que andaba muy acertado en
lo que deseaba y pedía, y que tal prosupuesto era propio y natural de los
caballeros tan principales como él parecía y como su gallarda presencia
mostraba; y que él, ansimesmo, en los años de su mocedad, se había dado a
aquel honroso ejercicio, andando por diversas partes del mundo buscando sus
aventuras, sin que hubiese dejado los Percheles de Málaga, Islas de Riarán,
Compás de Sevilla, Azoguejo de Segovia, la Olivera de Valencia, Rondilla de
Granada, Playa de Sanlúcar, Potro de Córdoba y las Ventillas de Toledo y
otras diversas partes, donde había ejercitado la ligereza de sus pies,
sutileza de sus manos, haciendo muchos tuertos, recuestando muchas viudas,
deshaciendo algunas doncellas y engañando a algunos pupilos, y, finalmente,
dándose a conocer por cuantas audiencias y tribunales hay casi en toda
España; y que, a lo último, se había venido a recoger a aquel su castillo,
donde vivía con su hacienda y con las ajenas, recogiendo en él a todos los
caballeros andantes, de cualquiera calidad y condición que fuesen, sólo por
la mucha afición que les tenía y porque partiesen con él de sus haberes, en
pago de su buen deseo.}}


\chapter{CG as a SAT-problem}
\label{chapterCGSAT}

In this chapter, we present CG as a Boolean satisfiability (SAT) problem,
and describe an implementation using a SAT-solver. 
This is attractive for several reasons: formal logic is
well-studied, and serves as an abstract language to reason about the
properties of CG. Constraint rules encoded in logic capture richer
dependencies between the tags than standard CG. 
%; much like in parallel CG, one rule is not a self-contained unit, but a piece in the puzzle.


%So far, we have presented CG and SAT as separate success stories: CG is easy to adopt, even for less-resourced languages, and achieves high F-scores; SAT is used in reducing difficult search problems into a low-level task.
%But is there a reason to combine the two? 

%CG lends itself well to a logical representation. It's all about expressing what is true (`SELECT') and false (`REMOVE'), under certain conditions (`IF').
Applying logic to reductionist grammars has been explored earlier by \cite{lager98,lager_nivre01}, but there has not been, to our knowledge, a full logic-based CG implementation; at the time, logic programming was too slow to be used for tagging or parsing. 
Since those works, SAT-solving techniques have improved significantly \cite{marques_silva2010}, and they are used in domains such as microprocessor design and computational 
biology---these problems easily match or exceed CG in complexity. 
In addition, SAT-solving brings us more practical tools, such as maximisation, which enables us to implement novel conflict resolution for parallel CG.


The content in this chapter is based on \cite{listenmaa_claessen2015}.
As in the original paper, we present a translation of CG rules into logical formulas, and show how to encode it into a SAT-problem.
This work is implemented as an open-source software SAT-CG\footnote{\url{https://github.com/inariksit/cgsat}}. It uses the high-level library SAT+\footnote{\url{https://github.com/koengit/satplus}}, which is based on MiniSAT \cite{een04sat}.
We evaluate SAT-CG against the state of the art, VISL CG-3.
The experimental setup is the same, but we ran the tests again for this thesis: since the writing of  \cite{listenmaa_claessen2015}, we have optimised our program and fixed some bugs; likewise, VISL CG-3 has been updated, and executes faster than what we report in the earlier paper. 
While the absolute numbers have changed, the relative differences between the running time and F-scores remain as in the paper.


\section{Previous work: CG in logic}\label{encoding-in-logic}

\input{chapters/cg-sat-relatedwork}


\section{CG as a SAT-problem}
\label{sec:CGSAT}

In this section, we translate the disambiguation of a sentence into a SAT-problem.
We demonstrate our encoding with an example in Spanish, shown in Figure~\ref{fig:laCasaGrande}: {\em la casa grande}. % (`the big house'). 
The first word, {\em la}, is ambiguous between a definite article (`the') or an object pronoun (`her'), and the second word, {\em casa}, can be a noun (`house') or a verb form (`marries').
The subsegment {\em la casa} alone can be either a noun phrase, $\laDet \ \casaN$ 
`the house'  or a verb phrase $\laPrn \ \casaV$   `(he/she) marries her'. 
However, the unambiguous adjective, {\em grande} (`big'), disambiguates the whole segment into a noun phrase: `the big house'.

Firstly, we translate input sentences into variables and rules into clauses.
Secondly, we disambiguate the sentence by asking for a solution. 
Finally, we consider different ordering schemes and conflict handling.


\subsection{Encoding the input}


\begin{figure}[h]
\centering
\begin{tabular}{p{0.6cm} l | c | c }
%\multicolumn{2}{c}{}
   & \textbf{Original~analysis} 
                & \textbf{Variables}
                              & \textbf{Default rule} \\ \hline
\t{"<la>"}   &   &            &  {\small \defRule} \\
  & \t{"el" 
  det def f sg}  & $\laDet$   &  \\
  & \t{"lo" 
  prn p3 f sg}   & $\laPrn$   &   $\laDet \vee \laPrn$ \\
\t{"<casa>"} &   &            &   \\
  & \t{"casa" 
  n f sg}        & $\casaN$   &  \\
  & \t{"casar"
   v pri p3 sg}  & $\casaV$   & $\casaN \vee \casaV$  \\
\t{"<grande>"} & &            & \\
  & \t{"grande" 
  adj mf sg}   & $\grandeAdj$ & $\grandeAdj$
\end{tabular}
\caption{Ambiguous segment in Spanish: translation into SAT-variables.}
\label{fig:laCasaGrande}
\end{figure}


% \begin{figure}[h]
% \centering
% \begin{verbatim}
% "<la>"
%         "el" det def f sg
%         "lo" prn p3 f sg
% "<casa>"
%         "casa" n f sg
%         "casar" v pri p3 sg
% "<grande>"
%         "grande" adj mf sg
% \end{verbatim}
% \caption{Ambiguous segment in Spanish.}
% \label{fig:laCasaGrande}
% \end{figure}

\paragraph{Reading}
The readings of the word forms make a natural basis for variables.
We translate a combination of a word form and a reading, such as \texttt{"<la>" ["el" det def f sg]}, into a variable $\laDet$, which represents the possibility that \la{} is a determiner. This example segment gives us five variables: $\{ \laDet , \laPrn , \casaN , \casaV,  \grandeAdj \}$, shown in \ref{fig:laCasaGrande}.

\paragraph{Cohort} As in the original input, the readings are grouped together in cohorts. We need to keep this distinction, for instance, to model {\sc select} rules and cautious context: 
\t{SELECT "casa" n} means, in effect, ``remove $\casaV$'', and \t{IF (-1C prn)} means ``if $\laPrn$ is true and $\laDet$ false''. 
%
Most importantly, we need to make sure that the last reading is not removed. Hence we add the default rule, \defRule, as shown in the third column of \ref{fig:laCasaGrande}. 
These disjunctions ensure that at least one variable in each cohort must be true.



\paragraph{Sentence}
In order to match conditions against analyses, the input needs to be structured as a sentence: the cohorts must follow each other like in the original input, indexed by their absolute position in the sentence. Thus when we apply \texttt{REMOVE v IF (-1 det)} to the cohort $2 \rightarrow [\casaN , \casaV]$, the condition will match on $\laDet$ in cohort 1.


\paragraph{Rule}

Next, we formulate a rule in SAT. A single rule, such as \texttt{REMOVE v IF (-1 det)}, is a template for forming an implication; when given a concrete sentence, it will pick concrete variables by the following algorithm.

\begin{enumerate}
\item Match rule against all cohorts
 \begin{itemize}
    \item[\la:] No target found
    \item[\casa:] Target found in $\casaV$, match conditions to \la
      \begin{itemize}
       \item Condition found in $\laDet$
       \item Create a clause: $\laDet \Rightarrow \neg \casaV \ $ `if \la{} is a determiner, \casa{} is not a verb'
      \end{itemize}
    \item[\grande:] No target found
  \end{itemize}
\item Solve with all clauses: 
  $\{ \ob{\laDet \! \vee \laPrn, \ \casaN \vee \casaV, \  \grandeAdj}^{\text{given by the default rule}}, \ 
      \ob{\laDet \Rightarrow \neg \casaV}^{\t{REMOVE v IF (-1 det)}} \}$
\end{enumerate}

In Appendix~\ref{appendix1}
, we have included a translation of {\sc remove} and {\sc select} rules, with all possible conditions in CG-2 \todo{check if all}. The following examples in this section do not require reading the appendix.

\subsection{Applying a rule}

%Finally, we have all we need to solve the disambiguation problem. Given the clauses presented in step 2, SAT-solver returns a model---this is our disambiguated sentence. 

Finally, we have all we need to disambiguate the segment: the sentence and the constraints encoded as SAT-variables and clauses. The SAT-solver returns a model that satisfies all the clauses presented in step 2.
We started off with all the variables unassigned, and required at least one variable 
in each cohort to be true. In addition, we gave the clause $\laDet \Rightarrow \neg \casaV$.
We can see with bare eye that this problem will have a solution; in fact, multiple ones, 
shown in Figure~\ref{fig:modelsOneRule}.
The verb analysis is removed in the first two models, as required by the presence of $\laDet$. However, the implication may as well be interpreted ``if $\casaV$ may not follow $\laDet$, better remove $\laDet$ instead''; this has happened in Models 3--4. 
We see a third interpretation in Model 5: $\casaV$ may be removed even without 
the presence of $\laDet$. This is possible, because $\laDet \Rightarrow \neg \casaV$ is only an implication, not an equivalence.

\begin{figure}[h]
\centering
$$\begin{array}{ c | c | c | c | c}
\textbf{Model 1}  & \textbf{Model 2}  & \textbf{Model 3} & \textbf{Model 4} & \textbf{Model 5} \\ \hline
 \laDet   &  \laDet  &         &        &        \\
          &  \laPrn  & \laPrn  & \laPrn & \laPrn \\
 \casaN   &  \casaN  & \casaN  &        & \casaN \\
          &          & \casaV  & \casaV &         \\
\grandeAdj & \grandeAdj & \grandeAdj & \grandeAdj & \grandeAdj \\

\end{array}$$
\caption{Possible models for \t{REMOVE v IF (-1 det)}.}
\label{fig:modelsOneRule}
\end{figure}


It seems like SAT-CG does worse than any standard CG implementation:
the latter would just remove the verb, not give 5 different interpretations for a single rule.
But there is power to this property. Now, we add a second rule: \texttt{REMOVE n IF (-1 prn)}, which will form the clause $\laPrn \Rightarrow \neg \casaN$. The new clause
%, together with $\laDet \Rightarrow \neg \casaV$, 
prohibits the combination $\laPrn \ \casaN$, which rules out three models out of five. The disambiguation is shown in Figure~\ref{fig:modelsTwoRules}.

\begin{figure}[h!]
\centering
$$\begin{array}{ c | c }
 \textbf{Model 1}  & \textbf{Model 2}  \\ \hline
 \laDet   &          \\
          &  \laPrn  \\
 \casaN   &          \\
          &  \casaV   \\
\grandeAdj & \grandeAdj \\

\end{array}$$
\caption{Possible models for \t{REMOVE v IF (-1 det)} and \t{REMOVE n IF (-1 prn)}.}
\label{fig:modelsTwoRules}
\end{figure}


After two rules, we only have two models: one with $\laDet \ \casaN$ and other with $\laPrn \ \casaV$. 
%This behaviour corresponds to FSIG: 
In fact, we have accidentally just implemented FSIG \cite{koskenniemi90}, introduced in Section~\ref{sec:ordering}: the rules act in parallel, and if the sentence cannot be fully disambiguated, the remaining uncertainty is modelled as a disjunction of all possible combinations of readings.
In contrast, a sequential CG engine applies each rule individually, and it cannot handle disjunction; its only operation is to manipulate readings in a cohort.
%remove \footnote{In CG-3, we can also add readings to a cohort, and cohorts to a sentence.} readings from a cohort. 
The CG engine would have just applied one of the rules---say, the first one, removed the verb and stopped there. If another rule later in the sequence removes the determiner, there is no way to restore the verb. 

To finish our FSIG example, let us add one more rule: \t{REMOVE v IF (1 adj)}, and the corresponding clause $\grandeAdj \Rightarrow \neg \casaV$. This clause will rule out Model~2 of Figure~\ref{fig:modelsTwoRules}, and we will get Model~1 as the unique solution. 
We can see another benefit in allowing connections between rules: none of the three rules has targeted \la{}, still it has become unambiguous. 

% Now, this is all very nice, but the present thesis is not called ``Implementing FSIG with a SAT-solver''. 
% However, understanding the translation of rules to implications is vital to the rest of this thesis, and FSIG provides, arguably, a simpler starting point.
% In the following sections, we will discuss the concepts of conflict resolution and ordering of the rules. 
% Firstly, we show two ways to handle conflicts in the parallel setting, 
% and secondly, we consider an alternative method for a sequential SAT-encoding.


% We have given a minimal description of SAT-based implementation. 
% Many details are left vague: Do we enforce that all readings that are not targeted by rules will resolve to true? How do we treat ordering? 


\subsection{Solving conflicts in the parallel scheme}
\label{sec:parallelScheme}

As described in Section~\ref{sec:ordering}, the parallel FSIG behaves differently than the 
sequential CG: the rules are dependent on each other, and the order does not matter.
This prevents too hasty decisions, such as removing $\casaV$ before we know the status of \la{}. 
However, ignoring the order means that we miss significant information in the rule set. 
The truth is that pure FSIG is very brittle: each and every rule in the set must fit together, without the notion of order. The rule sequence in Figure~\ref{fig:ruleOrder}---taken from an actual grammar\footnote{\url{http://link.to/grammar}}---will be well-behaved in a strict and sequential CG. Its behaviour will be less clear in a heuristic and sequential CG, but in FSIG, it will definitely cause a conflict, rendering the whole grammar useless.



The order clearly demonstrates the course of action: ``if \todo{Foo} is preceded by \todo{Bar}, then remove/select \todo{Baz}; then, move on to other rules; finally, if \todo{Foo} is still ambiguous, select/remove \todo{Quux}.'' 
Comparing the success of CG to FSIG, one may speculate that the sequential order is easier to understand---undeniably, its behaviour is more transparent. %As opposed to FSIG, the rules are ordered. As opposed to the heuristic order, the rules behave always the same way, regardless of the input.
If two rules target the same cohort, the first mentioned gets to apply, and removes the target. When the first rule has acted, the second rule is not even considered, because it would remove the last reading.


% In the carefully crafted examples, we have ignored the careful mode: \t{IF (-1C det)} `if the previous word is unambiguously determiner'. 
Ideally, both ways of grammar writing should yield similar results:
sequential CG rules are more imperative, and FSIG rules are more declarative.
But the problem of conflicts in FSIG still remains. 
In the following, we present two solutions: 
in the first one, we emulate ordering in choosing which clauses to keep, and in the second one, we maximise the number of rule applications. 





\begin{figure}[ht]
\centering
   \begin{verbatim}
   SECTION:good_rules
   REMOVE baz IF (-1 bar) ;

   SECTION:heuristic   
   REMOVE quux ;
   \end{verbatim}
\label{fig:ruleOrder}
\caption{Example from a XXX grammar}
\end{figure}
%




\paragraph{Emulating order} 

We keep the parallel base, but use ordering as information for solving conflicts.
This means that all the benefits of parallel execution still hold: the three rules, which all target \emph{casa}, may still disambiguate \emph{la}, without \emph{la} ever being the target.
If all the rules play well together, no rule applications need to be removed; 
however, in the case shown in Figure~\ref{fig:ruleOrder}, the clauses created by 
\todo{REMOVE quux} would be ignored, in favour of the clauses that are created by earlier rules.



% we keep the parallel base: the cohort vectors are not manipulated between the rule applications, thus the 100\textsuperscript{th} rule still accesses the same variables as the first rule.
%the cohorts are encoded as vectors of variables, and the rules form implications at each application.

In this modified scheme, we introduce the clauses to the SAT-solver one by one, 
and attempt to solve after each clause. If the SAT-problem after the 50$^{th}$ rule 
has a solution, we accept all the clauses created by rule 50, and commit to them.
If rule 51 causes a conflict, we prioritise the previous, well-behaving subset of
50 rules, and discard the conflicting clauses created by rule 51.

If a rule matches multiple cohorts, it creates a separate clause for each instance.
Thus, it is no problem if the rule causes a conflict in only one cohort---say, we 
have another potential Quux in the sentence, but there is no other rule which targets 
its other readings. We can discard only the conflicting instances: we prevent 
\todo{REMOVE quux} from applying to $foo$, but it still may apply to other $foo$s.


Assuming our rule set is \{\t{REMOVE v IF (-1 det)}, \t{REMOVE v IF (1 adj)}, \t{REMOVE n}\}, the revised algorithm goes as follows:


\begin{enumerate}
\item Apply \t{REMOVE v IF (-1 det)}
 \begin{itemize}
    \item Create a clause: $\laDet \Rightarrow \neg \casaV$
    \item Solve with previous clauses:
  $\{ \ob{\laDet \! \vee \laPrn, \ \casaN \vee \casaV, \  \grandeAdj}^{\text{default rule}}, \ 
      \ob{\laDet \Rightarrow \neg \casaV}^{\t{REMOVE v IF (-1 det)}} \}$
    \item Solution found: add new clause to the formula
 \end{itemize}
\item Apply \t{REMOVE v IF (1 adj)} 
 \begin{itemize}
    \item Create a clause: $\grandeAdj \Rightarrow \neg \casaV$
    \item Solve with previous clauses:
  $\{..., \laDet \Rightarrow \neg \casaV, \ 
      \ob{\grandeAdj \Rightarrow \neg \casaV}^{\t{REMOVE v IF (1 adj)}}  \}$
    \item Solution found: add new clause to the formula
 \end{itemize}
\item Apply \t{REMOVE n}
 \begin{itemize}
    \item Create a clause: $\neg \casaN$
    \item Solve with previous clauses:
      $\{ ..., \laDet \Rightarrow \neg \casaV, \ 
      \grandeAdj \Rightarrow \neg \casaV, \ 
      \ob{\neg \casaN}^{\t{REMOVE n}} \}$
  % $\{ \ob{\laDet \! \vee \laPrn, ...}^{\text{default rule}}, \ 
  %     \ob{\laDet \Rightarrow \neg \casaV}^{\t{REMOVE v IF (-1 det)}}, \ 
  %     \ob{\grandeAdj \Rightarrow \neg \casaV}^{\t{REMOVE v IF (1 adj)}}, \ 
    \item No solution: discard clause
 \end{itemize}
\end{enumerate}







\paragraph{Maximisation} 

Solving conflicts means that we have multiple rules that target the same reading, and we must choose which rule to apply.
Strict ordering substitutes the question with a simpler one: ``which rule comes first in the grammar?''
Heuristic rule order asks ``out of all the rules that target this cohort, which one has the best matching context?''
If the competitors are \cgrule{REMOVE n IF (-1 prn)} and \cgrule{REMOVE v IF (-1 det) (1 adj)}, then the second one will win. However, if the rules are both as good a match, which happens in Figure~\ref{fig:modelsTwoRules}, we need to resort to mere guessing, or fall back to ordering.

However, there is another question: ``Out of all the rules that target this cohort, which one is a best fit \emph{with other rules that will apply to this whole sentence}?''
Addressing this is beyond the means of previous FSIG implementations \todo{check if so}, but with a SAT-solver, we can answer this question. 


Each rule application to a concrete cohort produces a clause,
and the whole rule set applied to the whole sentence produces 
a large formula. In an ideal case, all the rules are well-behaved, 
and the whole formula is satisfiable. However, if the whole formula 
is unsatisfiable, we may still ask for an assignment that satisfies 
the maximum number of the clauses; that is, rule applications. 
If the grammar is good, we hope that the interaction between 
the appropriate rules would make a large set of clauses that 
fit together, and the inapplicable rule would not ``fit in''.

%In the SAT-world, this means that the largest number of satisfiable clauses would include the group of well-fitting rules, and leave the odd rule out.
% The order-based heuristic in the traditional CG is replaced by a more
% holistic behaviour: if the rules conflict, discard the one that seems
% like an outlier.

We keep the rule set \{\t{REMOVE v IF (-1 det)}, \t{REMOVE v IF (1 adj)}, \t{REMOVE n}\}.
Now the procedure goes as follows:

\begin{enumerate}
\item Apply \t{REMOVE v IF (-1 det)}
 \begin{itemize}
    \item Create a clause: $\laDet \Rightarrow \neg \casaV$
 \end{itemize}
\item Apply \t{REMOVE v IF (1 adj)} 
 \begin{itemize}
    \item Create a clause: $\grandeAdj \Rightarrow \neg \casaV$
 \end{itemize}
\item Apply \t{REMOVE n}
 \begin{itemize}
    \item Create a clause: $\neg \casaN$
 \end{itemize}

\item Solve with all clauses:
  $\{ \ob{\laDet \! \vee \laPrn, ...}^{\text{default rule}}, \ 
      \ob{\laDet \Rightarrow \neg \casaV}^{\t{REMOVE v IF (-1 det)}}, \ 
      \ob{\grandeAdj \Rightarrow \neg \casaV}^{\t{REMOVE v IF (1 adj)}}, \ 
      \ob{\neg \casaN}^{\t{REMOVE n}} \}$
\item No solution for all clauses: try to find a solution that satisfies maximally many clauses; however, default rule cannot be overridden.
\end{enumerate}

Similarly to the previous, order-based scheme, we create a clause for each 
instance of the rule application. In the case of a conflict, we can 
only discard the clauses targeting the offending cohort, but the rule may apply 
elsewhere in the sentence.


The problem of satisfying maximum amout of clauses is known as \emph{Maximum Satisfiability} (MaxSAT).
Whereas SAT is a decision problem, MaxSAT is an optimisation problem.
However, optimisation can be expressed as a sequence of decision problems:
first, we compute a solution, then we add a constraint ``the solution must be better than the one just found'', and ask for another one. 
This process repeats until a better solution cannot be found; then we accept the 
latest solution.

Now let us define how is one solution ``better'' than other. 
To start, remember that implications can be translated into disjunctions: 
$\laDet \Rightarrow \neg\casaV$ is equivalent to $\neg\laDet \vee \neg\casaV$. 
We can associate helper variables to these clauses with a simple trick.
For each clause, we create a new variable $v$. Then we add 
the negation of this variable to the corresponding clause, 
so that it becomes $\neg{}v \vee \neg\laDet \vee \neg\casaV$.
Then, we ask for a solution where maximally many of these $v$s are true,
and the question for improvement becomes ``can we make any more of the $v$s true''?

What happens if a given $v$ is false? The SAT-solver has received all its clauses 
with the added $\neg{}v$. 
%Now, if we know that $\neg{}v$, then we know that the whole disjunction is true, without even considering the rest.
%If we have a long disjunction and we know its first argument is true, then we see
%immediately that the whole disjunction is true.
Now, if $v$ is false, that is, $\neg{}v$ is true, then we know that 
the whole disjunction $\neg{}v \vee \neg\laDet \vee \neg\casaV$ is true.
This means that the SAT-solver can ignore the rest of the clause---the part that comes from the rule application.
%Since this part comes from the rule application, the effect of this trick with $v$ means that the original rule application can be ignored.
%It knows already that the whole disjunction is true, regardless if the original rule applies or not.
Conversely, if $v$ is true, then its negation $\neg{}v$ is eliminated from the clause, 
and the SAT-solver must take into account the original $\neg\laDet \vee \neg\casaV$.


How, then, are the helper $v$s maximised? 
For a problem with $n$ variables, there are $2^n$ possible assignments; 
it is not practical to try them all and choose the solution with most $v$s.
\formulation{However, there are methods for ordering different solutions, even before attempting to solve.
Then, if we have tried the solutions with 10 $v$s and failed, we know that no solution with 11 or more $v$s would work either. The algorithm used in SAT-CG is like the one in \cite{een06minisatplus}.}





% In the following, we show a modification to the parallel scheme, 
% where the clauses are added to the formula incrementally, 
% making sure that each new clause fits with the previously added.
% %This allows us to emulate order, but still keep the benefits from the parallel
% We contrast this with an alternative scheme; one that will be used in Chapter~\ref{chapterCGana}.



% \subsection{Main differences between SAT-CG/FSIG/Lager98-almost-CG and good old sequential CG}

% \paragraph{Rules disambiguate more}
% Considering our example phrase and rules, the standard CG implementation
% can only remove readings from the target word (\texttt{prn} or
% \texttt{det}). The SAT-based implementation interprets the rules as
% ``determiner and verb together are illegal'', and is free to take action that concerns also the word in the condition (\texttt{n} or \texttt{v}).

% This behaviour is explained by simple properties of logical formulae.
% When the rules are applied to the text, they are translated into
% implications: \texttt{REMOVE prn IF (1 n)} becomes $\casaN \Rightarrow \neg \laPrn$,
%  which reads ``if the \texttt{n} reading for \emph{casa} is true, then
%  discard the \texttt{prn} reading for \emph{la}''.
% Any implication $a\,\Rightarrow\,b$ can be represented as a disjunction
% $\neg a\,\vee\,b$; intuitively, either the antecedent is false
% and the consequent can be anything, or the consequent is true and the
% antecedent can be anything.
% Due to this property, our rule translates into the disjunction 
% $\neg\casaN  \vee  \neg \laPrn$,
% which is also equivalent to another implication, 
% $\laPrn \Rightarrow \neg \casaN$.
% This means that the rules are logically flipped: \texttt{REMOVE prn IF
%   (1 n)} translates into the same logical formula as  \texttt{REMOVE n
%   IF (-1 prn)}. 
% A rule with more conditions corresponds to many rules, each condition
% taking its turn to be the target. % of removal or selection.

% % \begin{itemize}
% % \item [] \texttt{REMOVE n IF (-1 prn) ;} \\
% %          \texttt{REMOVE v IF (-1 det) ;}
% % \end{itemize}

% % SAT-based approach gives identical results with both sets of rules,
% % whereas the standard CG would remove one reading from \emph{casa} and leave \emph{la} ambiguous.
% % Testing this property with more complex rules and larger rule sets remains to be done.

% %Rules with more conditions translate into many rules; rules with negation become complements (\texttt{(*)-X} for \texttt{NOT X}). Rules which require the condition to be unambiguously tagged, don't have an equivalent flip in the standard CG.
% % We did not expect much practical benefits, save for realising that some rules are bad by having to think further what it implies,


% \paragraph{Cautious context is irrelevant}
% % Rather than waiting for a word to get disambiguated, the SAT solver starts by 
% % making assumptions (e.g. ``\emph{casa} is a noun'') and working under them,
% % discarding the assumption if it doesn't lead to a model that satisfies
% % all constraints.

% Traditional CG applies the rule set iteratively:
% some rules fire during the first iteration, either because their
% conditions do not require cautious context, or because some words are
% unambiguous to start with. This makes some more words unambiguous, and
% new rules can fire during the second iteration.

% In SAT-CG, the notion of cautious context is irrelevant. Instead of
% removing readings immediately,  each rule generates a  number of
% implications, and the SAT solver tries to find a model that will satisfy them. 

% The SAT-based approach only removes readings after it has enough
% evidence to do that. From the grammar writer perspective, this removes
% a burden of having to decide whether the rule should be cautious or
% not---the SAT solver will only take action the surrounding context
% supports the decision.

% % \todo{Unordered rules: also not applicable anymore to apply rules
% % iteratively; there's no ``this rule doesn't fire now but will after
% % applying X and Y'', it's all just implications ``this rule will fire
% % if this is true'' and let the SAT solver find if those rules can apply
% % peacefully to the same input.}




\section{Experiments}
\label{sec:eval}

In this section, we report experiments on the two modifications to the parallel scheme: maximisation-based, and ordering by committing to a prefix of rules.
We evaluate the performance against VISL CG-3 in accuracy and running time; in addition, we offer some preliminary observations on the effect of grammar writing.


For evaluation, we measure the performance against the state-of-the-art CG parser VISL CG-3.
% both in terms of accuracy and execution time.
SAT-CG fares slightly worse for accuracy, and significantly worse for execution time.
The results are presented in more detail in the following sections.
We include evaluation for the two schemes presented in previous sections: 
%This suggests that our implementation should not compete with existing
%state-of-the-art, but rather it has value as a way of relating the CG
%formalism to wider context in the theory of computer science.
%In Section~\ref{sec:apps} we discuss more about possible applications.



\def\satcgMax{SAT-CG\textsubscript{Max}}
\def\satcgOrd{SAT-CG\textsubscript{Ord}}

\subsection{Performance against VISL CG-3}


\begin{table}[h]
%   \begin{tabular}{ c | c c | c c | c c }
%       & \multicolumn{2}{c}{19 rules}  
%                         &  \multicolumn{2}{c}{99 rules} 
%                                           & \multicolumn{2}{c}{261 rules} \\ 
%       & F-score & Time  & F-score & Time  & F-score & Time  \\ \hline
% \textbf{VISL CG-3}
%       & 82.6 \% & 4.2s  & ??? \% & 6.1s  & ??? \% & 10.7s \\
% \textbf{\satcgOrd}
%       & 81.5 \% & 22.1s & ??? \% &1m 15s & ??? \% & 2m 32s \\
% \textbf{\satcgMax}
%       & 79.2 \% & 39.7s & ??? \% &1m 34s & ??? \% & 2m 54s \\ 
\centering
\begin{tabular}{r | c c | c c | c c}
 
           & \multicolumn{2}{c}{\textbf{\satcgMax}} &  \multicolumn{2}{c}{\textbf{\satcgOrd}} & \multicolumn{2}{c}{\textbf{VISL CG-3}} \\ 
\textbf{\# rules} & F-score & Time       &  F-score & Time    & F-score & Time \\ \hline
      19   & 82.6 \% & ?          &  ? \%    & ?       &  ? \%    & ?\\ 
      99   & 81.5 \% & ?          &  ? \%    & ?       &  ? \%    & ?\\ 
      261  & 79.2 \% & ?          &  ? \%    & ?       &  ? \%    & ?\\ 
  \end{tabular}
\caption{F-scores and execution times for the subset of the Spanish grammar, tested on a gold standard corpus of 22,000 words.}
  \label{table:fscore}
\end{table}

We took a manually tagged
corpus\footnote{\url{https://svn.code.sf.net/p/apertium/svn/branches/apertium-swpost/apertium-en-es/es-tagger-data/es.tagged}}
containing approximately 22,000 words of Spanish news text, 
and a small constraint grammar\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}, produced independently of the authors.
% We ignored substitute rules, but kept subreadings and unification, even though our implementation doesn't handle them.
We kept only \textsc{select} and \textsc{remove} rules, which left us 261 rules.
With this setup, we produced an ambiguous version of the tagged
corpus, and ran both SAT-CG and VISL CG-3 on it.
In addition, we wrote a small grammar of 19 rules
To add middle ground, we chose a random subset of 99 rules from the original grammar: 
the purpose is to demonstrate the effect of the number of rules.

Table~\ref{table:fscore} shows F-scores and execution times for these three variants.\satcgMax is the parallel scheme with maximisation and
\satcgOrd is the parallel scheme where rule applications are introduced one by one.
\todo{Treating the original corpus as the gold standard, the disambiguation
by  VISL CG-3 achieves F-score of 82.6 \%, ordered SAT-CG 81.5 \%  and
unordered SAT-CG 79.2 \%.}

We did not test with other languages or text genres due to the lack of
available gold standard.




\subsection{Execution time}


\begin{table}[h]
  \centering
  \begin{tabular}{ r | c c c }
     \textbf{\# rules} & \textbf{\satcgMax}  & \textbf{\satcgOrd} & \textbf{VISL CG-3} \\ \hline
      19   & 39.7s &  22.1s   & 4.2s    \\ 
      99   & 1m34.1s & 1m14.9s & 6.1s   \\ 
      261  & 2m54.1s & 2m31.6s & 10.7s  \\ 
   \end{tabular}
  \caption{Execution times for subsets of the Spanish grammar, disambiguating Don Quijote (384,155 words).}
  \label{table:time}
\end{table}


\paragraph{Number of rules vs. number of words}

In addition to the 22,000-word corpus of news text,
we tested the performance by parsing Don Quijote (384,155 words) with
the same Spanish grammars as in the previous experiment. 
We added this experiment in order to stress test our implementation.
Don Quijote contains, on average, longer sentences: the news text has the average sentence length of 14.1 words, median 11 words and maximum 72 words; Don Quijote has, respectively, almost double for each of these numbers: average 25.8, median 22 and maximum 147 words.
For these numbers, we have included colons and semicolons as well as full stops, because
the grammar includes those as delimiters.
Table~\ref{table:time} shows execution times compared to VISL CG-3;


For both systems, the number of rules in the grammar affects the performance more than the word count.

\paragraph{Sentence length} 
In addition, we explored the effect of sentence length further.
When split by just full stops, the longest sentence in Don Quijote consists of 283 tokens\footnote{The longest sentence in Don Quijote: \ventero}, including punctuation. In Table~\ref{table:timeVentero}, we see this sentence parsed, on the right as a single unit, and on the left, we split it in three parts, at the semicolons. In Table~\ref{table:time}, it was already split by semicolons---as we can see, parsing the 283 tokens as a single unit takes more time than parsing the whole 384,155 tokens.
Even with the split, this sentence is responsible for good 16 \% of the parsing time of the whole novel. 


% 

\begin{table}[ht]
  \centering
  \begin{tabular}{ r | c c c | c c c }
       & \multicolumn{3}{c}{Whole sentence: 283 tokens} & \multicolumn{3}{c}{Split: x + 126 + y tokens} \\
     \textbf{\# rules} & \textbf{\satcgMax}  & \textbf{\satcgOrd} & \textbf{VISL CG-3} 
                                        & \textbf{\satcgMax}  & \textbf{\satcgOrd} & \textbf{VISL CG-3} \\ \hline
      19   & 12s    &  ?  & 0.021s   & 3.2s   & ? & 0.021s \\ 
      99   & ?      &  ?  & ?        & ?      & ? & ? \\ 
      261  & 11m 2s &  ?  & 0.052s   & 1m 47s & ? & 0.028s \\ 
   \end{tabular}
  \caption{Experiments with sentence length: parsing the longest sentence in Don Quijote. On the left, the complete 283-token sentence; on the right, split at semicolons into three parts.}
  \label{table:timeVentero}
\end{table}


From the SAT-solving side, maximisation is the most costly operation. 
Emulating order is slightly faster: maximisation is not needed, but
the solve function is performed after each clause.
In any case, SAT does not seem to be the bottleneck: with 261 rules,
the maximisation function was called 147,253 times, and with 19 rules,
132,255 times, but 
%However, with both rule sets, half of the sentences in Don Quijote
%needed less than 6 calls of maximise, and 75 \% of the sentences
%needed less than 14.
the differences in the execution times are much larger, which suggests
that there are other reasons for the worse performance. 
This is to be expected, as SAT-CG is a rather naive
proof-of-concept implementation with no optimisations.

\paragraph{Comparison to FSIG}

However, we notice that SAT-CG performs, in effect, a different task from VISL CG-3.
Perhaps a more accurate comparison would be against a state-of-the-art FSIG engine; 
unfortunately, there have been none since the 1990s \todo{cite!}.
To give some context, let us compare an FSIG engine from 1998 to CG engines from the same time.
\cite{voutilainen1998} reports the performance of a 3,500-rule CG: ``On a 266 MHz Pentium running Linux, EngCG-2 tags around 4,000 words per second''. In contrast, a 2,600-rule FSIG grammar is unable to find a correct parse in the allowed time, 100 seconds per sentence, for most sentences longer than 15 words.  
As another result for CG, \cite{tapanainen1999phd} reports a range between 2 and 20 seconds for sentences below 50 words; the average time for 15-word sentences is under 5 seconds.
%the execution time grows in a manner that follows $O(n log\!n)$.
Based on those figures, it would be interesting to run the same FSIG grammar with SAT-CG, and compare SAT-based approach for the same task.
The differences between CG and FSIG at the time are big; then again, we cannot ignore the differences in the grammars. \cite{voutilainen1998} points out that the 3,500-rule CG grammar contains local phenomena, whereas the 2,600-rule FSIG grammar has a more global scope, at the expense of processing efficiency.
For now, we cannot report very strong claims: compared to the situation in 1998, a modern SAT-based FSIG engine is closer to a modern CG engine, given that they operate on the same grammar.



\subsection{Effect on grammar writing}

Throughout the implementation of SAT-CG, we have predicted that the more declarative features would influence the way rules are written. 
We hoped to combine the best parts of CG and FSIG: because of the their mutual dependence, we would need less rules;
but conflicts would still be solved, and not render the whole grammar useless.
On the one hand, getting rid of ordering and cautious context could ease the task of the grammar writer, since it removes the burden of estimating the best sequence of rules and whether to make them cautious. On the other hand, lack of order can make the rules less transparent, and might not scale up for larger grammars.
Lacking an actual CG writer, it is hard to say whether this prediction holds or not.

In addition, our modifications to the brittleness of FSIG have let us run CGs as FSIG; something that previous FSIG implementations do not allow.
As an inverse experiment, it would be interesting to take some FSIG grammars, such as \cite{voutilainen1997fsig}, and run them with a CG engine, then compare the precision and recall. 
According to the previous literature, CG and FSIG rules are written 
The largest FSIG grammar is : \cite{voutilainen1998} describes the 2,600-rule English grammar as 


The results in Table~\ref{table:time} suggests that that when running grammars that are written
with the traditional CG in mind, SAT-CG loses with both ordering
strategies, but emulating order fares better.
We also tested whether SAT-CG outperforms traditional CG with a
small rule set. With our best performing and most concise
grammar\footnote{\url{https://github.com/inariksit/cgsat/blob/master/data/spa\_smallset.rlx}}
of only 19 rules, both SAT-CG and VISL CG-3  achieve a F-score of
around 85 \%. This experiment is very small and might be explained by
overfitting or mere chance, but it seems to indicate that rules that
work well with SAT-CG are also good for traditional CG.
It was possible to attain a rule set for which the unordered SAT-CG
works better (\textless0.5 \%), but it required trying
subsequences of the 19-rule set in a brute force manner---this hardly
presents any real life use case.

\todo{Not sure if this is worth mentioning \\
Out of interest, we also manually flipped a small grammar and tested it against the original.
The personal experiences of doing this was that ``wow, people don't really mean what they write''. Like if you're drawing a portrait, try to flip your paper upside down or look at it as a mirror image; it is easier to see if the ears are wonky or the eyes are all over the place.}


% Similar patterns were observed with small (\textless{}20) rule sets
% written by the authors; depending on the subset, SAT-CG and VISL CG-3 had
% a difference of at most $\Mypm$ 1.5 \%. 
% Introducing rules one by one up to 19, the
% performance improved in a very similar rate, with less than 0.5 \%
% difference between the systems at each new rule.
% We did not evaluate on other languages or text genres, due to lack of suitable test data.

% Additional tests could include plugging SAT-CG into Apertium
% translation pipeline, and comparing the translation quality.


The sequential application of traditional CG rules is good for
performance and transparency. When a rule takes action, the analyses
are removed from the sentence, and the next rules get the modified
sentence as input. 
% At the execution of rule, there is no way to go back to earlier rules and undo them. 
As a downside, there is no way to know which part comes directly from the
raw input and which part from applying previous rules.

In the examples, we have overlooked the C operator, and made a lot of noise how two rules without C can work together and who needs the damn C.
But SAT-CG is perfectly fine with C: it can start disambiguating, even if none of the rules is able to act on its own. 


% This corresponds loosely to the common design pattern in CGs, 
% where there is a number of rules with the same target: the more 
% secure rules are introduced first, with a catch-all rule as 
% the last resort, to be applied only if none of the previous rules 
% has met the conditions.

\section{Summary}

In this section, we started from the parallels between CG and logic:
the rules express what is true (`SELECT') and false (`REMOVE'), under certain conditions (`IF').
Plugging this into a SAT-solver, we get a working CG implementation---that is, parallel CG, known as FSIG. 
To alleviate the brittleness of FSIG, we developed two approaches to discard conflicting rule applications.
The first one is based on maximisation; trying to make as many clauses as possible apply, and the second one is based on order; introducing clauses one by one, and assuming that the previous clauses are true.


As a conclusion, SAT-solver provides an interesting alternative for a CG engine, but loses in practicality: performance and lack of transparency for grammar writers.
Perhaps the biggest contribution in this work has been to the FSIG community, with a practical implementation and ways to get around conflicts.
%CG and FSIG grammars are written in different styles
For the next chapter, we will use the benefits of SAT for analysing CGs.

%we found the alternative scheme in Section~\ref{sec:orderedScheme} promising for grammar analysis. 


% There is potential in the ``wait before acting'' property. No decision is lost---we can use SAT-encoding to analyse CGs, rather than execute them.



