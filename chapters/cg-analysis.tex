\def\newcite#1{\cite{#1}}

\chapter{Grammar analysis using SAT}
\label{chapterCGana}


In the previous chapter, we presented a tool.
In the current chapter, we will solve a problem.




Recall the design principles of CG from Section~\ref{sec:properties}: 
by design, the grammars are shallow and low-level.
There is no particular hierarchy between lexical, morphological,
syntactic or even semantic tags: individual rules can be written to address any
property, such as ``verb'', ``auxiliary verb in first person singular'',
or ``the word form \emph{sailor}, preceded by \emph{drunken} anywhere in the
sentence''. This makes it possible to treat very particular edge
cases without touching the more general rule: we would simply write
the narrow rule first (``if noun AND \emph{sailor}''), and introduce
the general rule (``if noun'') later.


However, this design is not without problems. As CGs grow larger, it
gets harder to keep track of all the rules and their interaction.
Despite this well-known issue, there has not been a tool that would help 
grammar writers to detect conflicting rules.
Following the idea further, the tool could give feedback that is not 
restricted to conflicts, but also other features that are helpful 
in the process of writing grammar.
Given the rules in Figure~\ref{fig:infrules}, a grammar writer may 
ask the following questions.



\begin{itemize}
\item Are all the Portuguese rules distinct? (e.g. \texttt{Para} and \texttt{De} may be included in \texttt{Prep})
\item Could two or more rules be merged? (e.g. \texttt{SELECT Inf IF -1 Prep OR Vai OR Vbmod ...})
\item What is the best order for the rules?
\item Can the Finnish rule on the left be rewritten in the form shown on the right?
\item Generate a sequence that triggers rule(s) $R$ but not rule(s) $R'$. 
\end{itemize}

%%%%%


\begin{figure}[t]
\ttfamily
\centering
\begin{tabular}{l | @{~~~} l  l}
SELECT Inf IF ... & \multicolumn{2}{c}{SELECT V + Prs/Imprt + Act + Neg IF ...} \\
~~(-1 Prep) (0C V) ;       & (*-1C Negv LINK NOT *1 Vfin)  & (NOT *-1 Niin OR Neg)  \\
~~(-1 Para OR De) (0C V) ; & (NOT 0 N) (NOT 0 Pron)        & (*-1C Negv \\
~~(-1C Vbmod) (0C V) ;     & (NOT *1 Neg) (NOT *-1 Neg)    &  ~~LINK NOT 0 Imprt \\
~~(-1C Vai) ;              & (NOT 0 Pass) (NOT *-1 Niin)   &  ~~LINK NOT *1 Vfin OR CLB?) \\
~~(-1C Vbmod) (0 Ser) ;    & (*-1C Negv LINK NOT *1 CLB?)  & (NOT 0 N OR Pron OR Pass) \\
~~(-1C Ter/de) ;           & (*-1C Negv LINK NOT 0 Imprt) ;  & (NOT *1 Neg) ; \\

\end{tabular}

\caption{Left: rules to select infinitive in Portuguese. 
        Right: two versions of a condition in Finnish.}

\label{fig:infrules}
\end{figure}

The chapter follows with introduction of related work: namely, corpus-based methods to aid grammar writing, and automatic optimisation of a complete, human-written grammar. We continue by presenting our solution, along with a working implementation, and finally, evaluate its performance.

\section{Related work}
\label{sec:CGanaRelated}

There has been previous research on corpus-based methods in manual grammar development \cite{voutilainen2004}, as well as optimisation of hand-written CGs~\cite{bick2013tuning}.
In addition, there is a large body of research on automatically
inducing rules, e.g. \cite{inducing_cg1996,lindberg_eineborg98ilp,lager01transformation,asfrent14}.
However, since our work is aimed to aid the process of hand-crafting rules, we omit those works from our discussion.


\paragraph{Corpus-based methods in manual grammar development}

Hand-annotated corpora are commonly used in the development of CGs, because they give immediate feedback whether a new rule increases or decreases accuracy \cite{voutilainen2004}.
% This helps the grammar writer to arrange the rules in appropriate sections, with safest and most effective rules coming first.
% However, this method will not notice a missed opportunity or a grammar-internal conflict, nor suggest ways to improve.
Voutilainen gives a detailed account about best practices of grammar writing and efficient use of corpora to aid the grammar development.
For a language with no free or tagset-compatible corpus available, \cite{tyers_reynolds2015} describe a method where they apply their rules to unannotated Wikipedia texts and pick 100 examples at random for manual check.

CG rules are usually arranged in sections, and run in the following manner. 
First apply rules from section 1, and repeat until nothing changes in the text. Then apply rules from sections 1--2, then 1--3 and so on, until the set includes all rules.
The best strategy is to place the safest and most effective rules in the first sections,
so that they make way for the following, more heuristic and less safe rules to act on.
A representative corpus is arguably the best way to get concrete numbers---how many times a rule applied and how often it was correct---and to arrange the rules in sections based on that feedback.

\cite{voutilainen2004} state that the around 200 rules are probably enough to resolve 50--75 \% of ambiguities in the corpus used in the development. 
This figure is nice and encouraging. It's very much thanks to Zipf's law: we can add rules that target the most frequent \emph{tokens}, thus disambiguating a high number of word forms.
However, this method will not notice a missed opportunity or a grammar-internal conflict, nor suggest ways to improve; neither does it guarantee a coherent whole of rules. 
While the coverage information is easy to obtain from a corpus, there is no tool that would aid grammar writers in including wide coverage of different linguistic phenomena.




\paragraph{Automatic optimisation of hand-written grammars }

The corpus-based method can tell the effect of each single rule at their place in the rule sequence, and leaves the grammar writer to make changes in the grammar.
\cite{bick2013tuning} modifies the grammar automatically, by trying
out different rule orders and altering the contexts of the rules. 
Bick reports error reduction of 7--15\% compared to the original grammars.

This is a valuable tool, especially for grammars that are so big that it's hard to keep track manually. A program can try all combinations whereas trying to make sense out of a huge set of rules would be hard for humans.
As a downside, the grammar writer will likely not know why exactly does the tuned grammar perform better.


\section{Analysing CGs}
\label{sec:sectionCGana}

We start by defining a conflict, and present requirements for a solution.
Then, we introduce a logical translation of sequential CG, corresponding to \cite{lager_nivre01}, and modify it into a SAT-problem about the \emph{original} sentence before applying the rules.
%this will tell us if it is possible for a senten
We refine our solution by restricting what kind of sentences we can create.
The whole method requires only a morphological lexicon, no corpus. 

\paragraph{Conflict}

We define \emph{conflict} as follows: a list of rules $R$ is in conflict with the rule $r$, if applying $R$ makes it impossible to apply $r$, regardless of input. 
Some examples of conflicts follow:

\begin{itemize}
\item If two equivalent rules $r$ and $r'$ occur in the grammar, the second occurrence will be disabled by the first
\item A list of rules $R$ selects something in a context, and $r'$ removes it
\item A list of rules $R$ removes something in a context, and $r'$ selects it
\item A list of rules $R$ removes something from the context of a rule $r'$, so $r'$ can never apply
\item A rule $r$ has an internal conflict, such as non-existent
tag combination, or contradictory requirements for a context word
\end{itemize}

This definition is very similar to the concept \emph{bleeding order} in generative phonology \todo{cite Kiparsky}; however, since we are talking about an explicit list of 
human-written rules, we include also rule-internal conflicts in our classification.
The conflicting (or ``bleeding'') $R$ can be a single rule or a list of rules: for instance, if one rule removes a verb in
context $C$, and another in context $\neg C$, together these rules
remove a verb in all possible cases, disabling any future rule that
targets verbs.

% While rule-internal conflicts can be detected by simpler means, taking
% care of rule interaction requires a {\em semantic} rather than a {\em
%  syntactic} analysis.
% In order to find effects of rule interaction, we must keep track of
% the possible sentences at each step. After each rule, we have two
% possibilities: the rule fires, or it does not fire. In case the rule does
% not fire, we have again two options: either its conditions are not met,
% or its target is the only remaining analysis. 

\paragraph{Solution}

How do we find out if a rule $r$ can act? 
We could apply the grammar to a large corpus and count the number of times 
each rule fires; if some rule never fires, we can suspect there is something wrong 
in the rule itself, or in the interaction with previous rules. 
But it can also be that $r$ just targets a rare phenomenon, and there was no sentence in the corpus that would trigger it.

Of course, corpus is an extremely useful tool for many questions in CG analysis.
A corpus can show which rules are the most used, or which rules give false analyses. 
Luckily, we have already methods for finding such statistics---the question 
about conflicts is orthogonal to those, and the solution needs to address only 
the part about conflicts, not about rules being frequent or accurate.
Therefore, we can take a more artificial approach. % generate our own sentences.
Remember the definition of conflict: ``applying $R$ makes it impossible to apply $r$ \emph{regardless of input}''.
In a nutshell, we are going to show the absence of a conflict by creating 
a sequence where $r$ can apply after $R$; conversely, we detect a conflict by 
showing that such sequence cannot be created.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\newVar{$\text{\em word}'_\textsc{ RD}$}
\def\oldVar{$\text{\em word}_\textsc{\,RD}$}
\def\eqdef{\Coloneqq}
\def\invConds{\text{invalid condition}}
\def\onlyTrgLeft{\text{only target left}}

\subsection{SAT-encoding of sequential CG}
\label{sec:orderedScheme}

In order to analyse real-life CGs, we need to model the semantics of sequential CG: 
rule application takes effect immediately, is irreversible, and only allows changes in the target.
There is no way around it: an analysis that treats \t{REMOVE~v ; REMOVE~v~IF~(-1~det)} the same as \t{REMOVE~v~IF~(-1~det) ; REMOVE~v} is worth nothing. The first one is a true conflict, because the first rule has a broader condition; it removes the verb in any possible case. The second rule order is possible: there can always be a sentence that has a verb, but is not preceded by a determiner, hence the first rule would not match, and the verb is saved for the second rule. 



We need to model the state of the sentence after each rule application. 
%Luckily, we do not need to reinvent the wheel\footnote{of course we did. like, who reads other people's papers.}: 
This is done in \cite{lager_nivre01}: they present CG as a composition of predicates, 
numbered by the rules in the grammar. Each predicate is applied to the result of the previous 
applications, and the ``do not remove last reading'' property is checked each time.

\todo{Make this transition smooth: \\
To be clear, the encoding in \cite{lager_nivre01} is \emph{not} a SAT-problem: 
there is no search involved, only manipulating Boolean expressions. 
%in the beginning, all the analyses present are true, and they are removed from a list
%We modify the scheme in two crucial ways:
%\begin{inparaenum}
%\item[(a)] 
%\end{inparaenum}
We present it in a way that is syntactically closer to what we have seen so far.
}

As before, a sentence is a vector of cohorts, which is, in turn, a vector of variables representing the readings of the current cohort.

Last time, we handled order by introducing clauses in order. All clauses operated on the same variables. Now, we must modify our approach. The state of a reading is expected to change during the execution of the rules---this is what rule application does. However, in the system of interconnected clauses, application of rule number 100 may not only affect the target of rule number 1, but conditions as well. 
A reading may be removed from a symbolic word between rules 1 and 100, if there is a rule that targets it among r2--r99. But if r1 requires w1 to be a noun, and r100 requires it to not be noun, this condition of r100 is not enough to make w1 change its nounhood at random.

We solve this problem by creating a new variable for a reading each time when it is targeted. The symbolic word is created with \t{w1<n>}, \t{w1<v>}, and after the first rule that targets nouns for removal (either \t{REMOVE n} or \t{SELECT $\neg$n}), we create \t{w1'<n>}. We allow \t{w1<n>} to be true and \t{w1'<n>} be false; both true; both false, but not \t{w1<n>} to be false and \t{w1'<n>} true. In other words, a reading may be in place all the time; absent all the time; or in place earlier and get removed later. No other changes are allowed.

%But now, we assume that each variable starts off as true: 
%all those readings are true at the ``first round'', and the goal is to make more and more readings false, as more rules are applied.
At each rule application, we create a new variable \newVar{} for each targeted reading \oldVar{}.
The new variable \newVar{} is true iff 

\begin{figure}[h]
\begin{tabular}{l l}
(a) \oldVar{} was true, and 
                             & (b) the rule cannot apply: this can be because \\
                                     & ~~~~-- its conditions do not hold, or \\
                                     & ~~~~-- it would remove the last reading.
\end{tabular}
\end{figure}

\noindent In another notation, application of \t{REMOVE v IF (-1 det)} looks like the following:

$$\begin{array}{r l}
\text{New variable } \casa{}'_\v 
      & \Leftrightarrow \casaV\ 
        \wedge\: ( \; \ob{\neg\laDet}^{\invConds} 
        \vee  \ob{(\casaV \wedge \neg \casaN) }^{\onlyTrgLeft} ) \\
\end{array}$$


\noindent After the application, we replace each \oldVar with \newVar in the sentence vector; those variables that are not touched by the rule, will be carried over to the next round unchanged. A sentence vector may look like following after two rules which target verbs and one which targets nouns:


%\begin{figure}
$$\begin{array}{l @{~\rightarrow~} l}
1 & \{\laDet, \laPrn \} \\
2 & \{\casa{}''_\v, \casa{}'_\n \} \\
3 & \{ \grandeAdj \} \\
\end{array}$$
%\label{fig:after3Rules}
%\end{figure}

\todo{Write rest of this section; merge with the stuff in paper; make a nice example!}

% Now, we may ask, why is this not a SAT-problem? %We sure talk about Boolean values and build formulas.
% The answer lies in the first step, where all variables start off as true. All the new variables, created from the rule applications, get their value immediately: $\casa{}'_\v \Leftrightarrow \grandeAdj \wedge \neg (\casaV \wedge \neg \casaN)$ just translates into $\casa{}'_\v \Leftrightarrow True \wedge \neg (True \wedge \neg True)$. There is nothing to decide, no search whatsoever; this problem is even simpler than the animal problem, where we at least had some choices to make.

% Can we find something in this setup that would make an interesting SAT-problem? 
% If this question is too intimidating, let us just ask ``can we make it into \emph{any} kind of SAT-problem''? 
% Forget all ambition, just ask what we \emph{can} do.
% We can go back to the first step: instead of all variables starting off as true, we can keep them unassigned. If we do that, where is the uncertainty?
% All the other variables computed along the way depend on the original variables, so the question to the SAT-solver becomes: \emph{``Which readings were originally true?''}

% Huh. That is an interesting question on its own right, but not very useful for a CG engine: we do know which readings were originally true, and we do not want the SAT-solver to be extra helpful and remove something that is targeted by no rule. But in Chapter~\ref{chapterCGana}, the new setup will allow us to find conflicts in the grammar, approximate generation and answer questions about expressivity. 
% For the remainder of this chapter, however, we keep the parallel scheme and wrap up what we have started. 


\subsection{Preliminaries}

Our analysis operates on a rule $r$, which is preceded by a list of rules $R$, and is concerned with answering the following question: ``Does there exist an input sentence $S$ that can trigger rule $r$, even after passing all rules $R$ that came before $r$?''
Instead of concrete sentences from a corpus, we apply rules on {\em symbolic sentences}:
every cohort contains every possible reading, and rule applications are responsible for shaping the sentence into a concrete one.

Before we can do any analysis any of the rules, we need to find out what the set of all possible readings of a word is. We can do this by extracting this information from a lexicon, but there are other ways too. In our experiments, the number of readings has ranged from about 300 to about 9000. 

Furthermore, when we analyse a rule $r$, we need to decide the {\em width} $w(r)$ of the rule $r$: How many different words should there be in a sentence that can trigger $r$? Most often, $w(r)$ can be easily determined by looking at how far away the rule context indexes in the sentence relative to the target. For example, in the rule mentioned in the introduction, the width is 2.

If the context contains a \verb!*! (context word can be anywhere),
we may need to make an approximation of $w(r)$, which may result in false positives or negatives later on in the analysis.
%we create sentences that are up to 3 words wider than the context without \verb!*!, and try them all.



\subsection{Creating realistic readings}
\label{sec:realistic_readings}

Earlier we have shown an example with 5 readings (``det def'', ``noun sg'', ...). In a realistic case, we operate between hundreds and thousands of possible readings. 
%This is very much dependent on language: the simplest language we tested was Dutch, with 336 readings. The most complex was 
In order to find the set of readings, we expand a morphological lexicon\footnote{We used the lexica from Apertium, found in \url{https://svn.code.sf.net/p/apertium/svn/languages/}.}, ignore the word forms and lemmas, and take all distinct analyses. 
However, many grammar rules target a specific lemma or word form.
A simple solution is to retain the lemmas and word forms only for those entries where it is specified in the grammar, and otherwise leave them out. For example, the Dutch grammar contains the following rule:

\begin{itemize}
 \item[] \texttt{REMOVE ("zijn" vbser) IF (-1 Prep) (1 Noun) ;}
\end{itemize}

This hints that there is something special about the verb \emph{zijn}, compared to the other verbs. Looking at the lexicon, we find \emph{zijn} in the following entries:

\begin{itemize}
 \item[] 
\begin{verbatim}zijn:zijn<det><pos><mfn><pl>
zijn:zijn<det><pos><mfn><sg>
zijn:zijn<vbser><inf>
zijn:zijn<vbser><pres><pl>
\end{verbatim}
\end{itemize}

Thus we add special entries for these: in addition to the anonymous
``det pos mfn pl'' reading, we add ``\emph{zijn} det pos mfn pl''. 
The lemma is treated as just another tag.

 However, for languages with more readings, this may not be feasible. For instance, Spanish has a high number of readings, not only because of many inflectional forms, but because it is possible to add 1--2 clitics to the verb forms.
The number of verb readings without clitics is 213, and with clitics 1572.
With the previously mentioned approach, we would have to duplicate 1572 entries for each verb lemma. Even ignoring the clitics, each verb lemma still adds 213 new readings.

The readings in a grammar can be underspecified: for example, the rule
\texttt{REMOVE (verb sg) IF (-1 det)} gives us ``verb sg'' and ``det''.
In contrast, the lexicon only gives us fully specified readings, such
as ``verb pres p2 sg''. We implemented a version where we took
the tag combinations specified in the grammar directly as our
readings, and we could insert them into the symbolic sentences as well.
The shortcut works most of the time, but if we only take the readings
from the grammar and ignore the lexicon, it is possible to
miss some cases: e.g. the rule \texttt{SELECT Pron + Rel IF (0 Nom)} 
may require ``pron rel nom'' in one reading, but this method only gives
``pron rel'' and ``nom'' separately. 

In addition, we found that the tag lists in the grammars sometimes
contain errors, such as using a nonexistent tag or using a wrong level
in a subreading. If we accept those lists as readings, we will
generate symbolic sentences that are impossible, and not discover
the bug in the grammar.
However, if we are primarily interested in rule interaction, then using
the underspecified readings from the grammar may be an adequate solution.

%In fact, it may even catch interaction conflicts between rules which have an internal conflict


\subsection{Creating realistic ambiguities}




In the previous section, we have created realistic \emph{readings}, by simply hardcoding legal tag combinations into variables. The next step in creating realistic \emph{ambiguities} is to constrain which readings can go together. For instance, the case of \emph{zijn} shows us that ``determiner or verb'' is a possible ambiguity. In contrast, there is no word form in the lexicon that would be ambiguous between an adjective and a comma, hence we do not want to generate such ambiguity in our symbolic sentences.

\begin{center}
\begin{tabular}{c|c|c|c|c}


            & n nt sg  & n f pl  & vblex sep inf & det pos mfn  \\ \hline
uitgaven    & 0        & 1       & 1             & 0    \\ 
toespraken  & 0        & 1       & 1             & 0    \\ 
haar        & 1        & 0       & 0             & 1    \\ 


\end{tabular}
\end{center}

We solve the problem by creating \emph{ambiguity classes}: groups of readings that can be ambiguous with each other. 
We represent the expanded morphological lexicon as a matrix, as seen
above: word forms on the rows and analyses on the columns. Each
distinct row forms an ambiguity class. For example, one class may
contain words that are ambiguous between plural feminine nouns and
separable verb infinitives; another contains masculine plural adjectives 
and masculine plural past participles.
Then we form SAT-clauses that allow or prohibit certain combinations. These clauses will interact with the constraints created from the rules, and the end result will be closer to real-life sentences.

Our approach is similar to \newcite{cutting_etal92}, who use ambiguity classes instead of distinct word forms, in order to reduce the number of parameters in a Hidden Markov Model. They take advantage of the fact that they don't have to model ``bear'' and ``wish'' as separate entries, but they can just reduce it to ``word that can be ambiguous between noun and verb'', and use that as a parameter in their HMM. 
%We can do a similar thing by saving a list of words with each ambiguity class. For example, we map the ambiguity class ``feminine plural noun or a separable verb infinitive'' to the list of word forms \{``uitgaven'', ``toespraken''\}, and then, if we generate such reading for our symbolic word, we can give one of these words as an example word.

There are two advantages of restricting the ambiguity within words.
Firstly, we can create more realistic example sentences, which should help the grammar writer.
Secondly, we can possibly detect some more conflicts. Assume that the grammar contains the following rules:

 \begin{itemize}
 \item[] 
\begin{verbatim}
 REMOVE adj IF (-1 aux) ;
 REMOVE pp  IF (-1 aux) ;
 \end{verbatim}
 \end{itemize}

 With our symbolic sentence, these rules will be no problem; to apply the latter, we only need to construct a target that has a realistic ambiguity with a past participle; the adjective will be gone already.
However, it could be that past participles (pp) only ever get confused with adjectives---in that case, the above rules would contradict each other.
 By removing the adjective reading, the first rule selects the past participle reading, making it an instance of ``$r$ selects something in a context, $r'$ removes~it''. 
The additional constraints will prevent the SAT-solver from creating an ambiguity outside the allowed classes, and such a case would be caught as a conflict.

\subsection{Use cases}

In the beginning of this chapter, we gave a list of questions, regarding the rules in Figure~\ref{fig:infrules}. After describing our implementation, we return to these questions, and explain how a SAT-solver can answer them.


\paragraph{Are all the rules distinct?} We gave the example of two rules in the Portuguese grammar, where one had the condition \t{(-1 Prep)} and the other \t{(-1 Para OR De)}. A grammarian who has some knowledge of Portugues could tell that {\em para} and {\em de} are both prepositions, so these two rules seem to do almost the same thing.

How can we verify this? To start, we apply all candidate rules to the same initial symbolic sentence. For the resulting symbolic sentence, we can ask for solutions with certain requirements.
For instance, ``give me a model where a reading $a \notin \t{Inf}$ is true''.
There are less such models allowed after \t{SELECT Inf IF (-1 Prep)}
---if a non-infinitive reading is in place, it means that the previous word cannot be any determiner---
and they are all in the set of models allowed \t{SELECT Inf IF (-1 Para OR De)}. Thus, we can show that the first rule implies the second. It is in the hands of the grammar writer to decide whether to keep the stronger or the weaker rule, or if they should be placed in different sections.


\paragraph{Could two or more rules be merged?} This example concerns two or more rules with the same target but different conditions in the same position. 
All the six rules in Figure~\ref{fig:infrules} have conditions in position -1; four of them also in 0. Some of them have an additional careful context, and some do not.
A grammar writer might think that \t{IF (-1C Vbmod (0C V))} and \t{IF (-1C Vbmod) (0 Ser))} are very similar, and could be merged into \t{SELECT Inf IF (-1C Vbmod) (0C V OR Ser)}. But could there be some unintended side effects in merging these two rules? Why is there a C in the first rule, but not in the second rule? 

The procedure is similar to the previous one. We initialise two symbolic sentences; on one we run the original rules in a sequence, and on the other the merged rule. 
Then, we can perform tests on the resulting symbolic sentences. Assuming that the ambiguity class constraints are in place, the result may show that C is not needed for the condition about {\em ser}, simply because {\em ser} is not ambiguous with anything else in the lexicon; hence the merged rule can safely have the condition \t{(0C V OR Ser)}. However, if {\em ser} can be ambiguous with something else, then this merged rule is stricter than the two original rules. If the grammar writer is still unsure whether there would be any meaningful sequences that would be missed, they can ask the SAT-solver to generate all those cases---this requires that the ambiguity classes are in place, \formulation{otherwise the number of solutions would be exponential on all possible readings in the lexicon.} 


\paragraph{What is the best order for the rules?}
We can generate variants of the rule order, and run all of them to an initial symbolic sentence. Then, for the resulting symbolic sentences, we can query which one is the most restricted, that is, has the least models.

As a variant, we may be interested in the placement of just one rule in the sequence.

\paragraph{Generate a sequence that triggers rule(s) $R$ but not rule(s) $R'$.}
For any given rule, we can extract three requirements that would make it trigger. We illustrate them for the rule \t{REMOVE v IF (-1 det)}. In order to trigger the rule, the sequence must have
\begin{itemize}
\item target readings: at least one reading with the \emph{v} tag in the target position
\item conditions: at least one reading with the \emph{det} tag in the cohort preceding the target
\item ambiguity: at least one reading without the \emph{v} tag in the target position; if there are only \emph{v}-readings in the target, the would not fire.
\end{itemize}

We can extract these requirements from both the rules that must be triggered, and the rules that must be not triggered. Then, we can request a solution from the SAT-solver.

\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}}},
with 59 rules; 
Spanish\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}},
with 279 rules; and 
Finnish\footnote{\scriptsize{\url{https://github.com/flammie/apertium-fin/raw/master/apertium-fin.fin.rlx}}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results for Dutch and Spanish are shown in Table~\ref{table:res},
and the results for Finnish in Table~\ref{table:resFin}.

A natural follow-up evaluation would be to compare the accuracy of the
grammar in the original state, and after removing the conflicts found
by our tool. We aim to perform such evaluation, when the toolset is more mature.

% Unfortunately, we did not have time to perform such
% evaluation, and in addition, we only have gold standard corpus (20~000~words) for Spanish.



\begin{table}[]
\centering
\begin{tabular}{|p{2.84cm}|p{1cm}|p{1.15cm}|p{1.55cm}|}
%\begin{tabular}{|p{2.9cm}|p{0.95cm}|p{1.15cm}|p{1.55cm}|}

\hline
                   & \textsc{nld}  & \textsc{spa}  & \textsc{spa}~\textsuperscript{sep.~lem.} \\ \hline
\# rules           & 59            & 279       & 279     \\ \hline
\# readings        & 336           & 3905      & 1735    \\ \hline
\# true~positives~\textsuperscript{AC}% with~amb.~classes
                   & 7             & 45        & 44      \\ 
{\small (internal~+~interaction)}
                   & {\small
                      (6~+~1)}     & {\small 
                                    (21~+~24)} & {\small (20~+~24)} \\ \hline

\# true~positives~\textsuperscript{no AC}  & 7             & 43        & 42      \\ 
{\small (internal~+~interaction)}
                   & {\small
                      (6~+~1)}     & {\small 
                                    (18~+~25)} & {\small (17~+~25)} \\ \hline
\# false positives 
                   & 0             & 0        & 1  \\ \hline

\clock{} with amb. 
           classes & 7 s        & 1h 46m   &  23 min  \\ \hline

\clock{} no amb. 
           classes & 3 s        & 44 min       & 16 min     \\ \hline 


\end{tabular}
\caption{Results for Dutch and Spanish grammars.}
\label{table:res}
\end{table}




The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that nearly all detected conflicts were
true positives---we found one false positive and one false negative,
when using a shortcut for the Spanish grammar. 
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not systematically check for false negatives in any of the
grammars, but we kept track of a number of known tricky cases; mostly
rules with negations and complex set operations.
As the tool matures and we add new features, a more in-depth analysis
will be needed.

\subsection{Dutch} The Dutch grammar had two kinds of errors: rule-internal and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological lexicon, but it appeared in one rule as the main reading. 

There was one genuine conflict with rule interaction, shown below:

\begin{itemize}
\item[\textsc{d$_1$.}] 
\begin{verbatim}REMOVE Adv IF (1 N) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 N) ;
\end{verbatim}
\end{itemize}

% These two rules both remove an adverb, but the first one has a broader condition:
% remove adverb if followed by a noun. In contrast, the second rule has a stricter condition: only remove adverb if it is preceded by a determiner, the adverb itself is ambiguous with an adjective, and followed by a noun. 
These two rules share a target: both remove an adverb.
The problem is that the first rule has a broader condition than the second, hence the second will not have any chance to act. 
If the rules were in the opposite order, then there would be no problem.



We also tested rules individually, in a way that a grammar writer might use our tool when writing new rules.
The following rule was one of them:

\begin{itemize}
\item[\textsc{d$_2$.}] 
\texttt{SELECT DetPos IF (-1 (vbser pres p3 sg)) (0 "zijn") (1 Noun);}
\end{itemize} 

As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require
 \emph{zijn} to be in the same reading with the target \emph{DetPos}.
It just means that at index 0,
there is a reading with any possessive determiner, and a reading with any \emph{zijn}.
However, the intended action is to select a ``det pos \emph{zijn}'' all in one reading;
this is expressed as \texttt{SELECT DetPos + "zijn"}.
In contrast, the 0-condition in example~\textsc{d$_1$} is used correctly:
the adjective and the adverb are supposed to be in different readings.


Can we catch this imprecise formulation with our tool? The SAT-solver will not mark it as a conflict (which is the correct behaviour). But if we ask it to generate an example sequence, the target word may be either of the following options. Seeing interpretation a) could then direct the grammar writer to modify the rule.

\begin{itemize}
\item[a)] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    zijn<vbser><inf>
\end{verbatim}

\item[b)] \begin{verbatim}
"w2"
    zijn<det><pos><mfn><pl>
\end{verbatim}
\end{itemize}


We found the same kind of definition in many other rules and grammars.
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used. As a possible extension, we could automatically merge the 0-condition into the target reading, then show the user this new version, along with the original, and ask which one was intended.

% In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.

\subsection{Spanish} The Spanish grammar had proportionately the
highest number of errors. The grammar we ran is like the one
found in the Apertium repository (linked on the previous page), 
apart from two changes: we fixed some typos (capital O for 0) in order to make it compile, and
commented out two rules that used regular expressions, because we did not implement the support for them yet.
For a full list of found conflicts, see the annotated log of running our program in \url{
https://github.com/inariksit/cgsat/blob/master/data/spa/conflicts.log}. 


We include two versions of the Spanish grammar in Table~\ref{table:res}: in column \textsc{spa}, we added the lemmas and word forms as described in Section~\ref{sec:realistic_readings}, and in column \textsc{spa}\textsuperscript{sep.~lem.}, we just added each word form and lemma as individual readings, allowed to combine with any other reading. 
This latter version ran much faster, but failed to detect an internal conflict for one rule, and reported a false positive for another. 
% FAIL TO DETECT: SELECT:pr_cnjadv_3 cnjadv IF (0 "después de") 
% FALSE POSITIVE: SELECT:este_1 det IF (0 "este" + (m sg)|(m sp)|(mf sg)|(mf sp))

When we added ambiguity class constraints, we found three more internal conflicts.
Interestingly, the version with ambiguity classes fails to detect an interaction conflict, which the simpler version reports, because one of the rules is first detected as an internal conflict. 
We think that neither of these versions is a false positive or negative; it is just a matter of priority. Sometimes we prefer to know that the rule cannot apply, given the current lexicon. 
However, we may know that the lexicon is about to be updated, and would rather learn about all potential interaction conflicts.

As an example of internal conflict, there are two rules that use \texttt{SET Cog = (np cog)}: the problem is that the tag ``cog'' does not exist in the lexicon. As another example, four rules require a context word tagged as NP with explicit number, but the lexicon does not indicate any number with NPs. It is likely that this grammar has been written for an earlier version, where such tags have been in place.
One of the conflicts that was only caught by the ambiguity class constraints had the condition \texttt{IF (1 Comma) (..) (1 CnjCoo)}. The additional constraints correctly prevent commas from being ambiguous with anything else.

As for the 25 interaction conflicts, there were only 9 distinct rules that rendered 25 other rules ineffective.
In fact, we can reduce these 9 rules further into 3 different groups: 4 + 4 + 1, where the groups of 4 rules are variants of otherwise identical rule, each with different gender and number.
An example of such conflict is below (gender and number omitted for readability):

\begin{itemize}
\item[\textsc{s$_{1}$.}] 
\begin{verbatim}
# NOM ADJ ADJ
SELECT A OR PP IF (-2 N) (-1 Adj_PP) (0 Adj_PP) (NOT 0 Det);

# NOM ADJ ADJ ADJ
SELECT A OR PP IF (-3 N) (-2 N) (-1 Adj_PP) (0 Adj_PP) (NOT 0 Det);
\end{verbatim}
\end{itemize}


In addition, the grammar contains a number of set definitions that were never
used. Since VISL CG-3 already points out unused sets, we did not add such
feature in our tool. However, we noticed an unexpected benefit when
we tried to use the set definitions from the grammar directly as our
readings: this way, we can discover inconsistencies even in
set definitions that are not used in any rule.
For instance, the following definition requires the word to be all of
the listed parts of speech at the same time---most likely, the grammar writer meant 
OR instead of +:
\begin{itemize}
\item[\textsc{s$_2$.}] 
\texttt{SET NP\_Member = N + A + Det + PreAdv + Adv + Pron ;}
\end{itemize}

If it was used in any rule, that rule would have been marked as
conflicting. We noticed the error by accident, when the program
offered the reading \texttt{w2<n><adj><det><preadv><adv><prn>}
in an example sequence meant for another rule.


As with the Dutch grammar, we ran the tool on individual rules and
examined the sequences that were generated. None of the following was
marked as a conflict, but looking at the output indicated that there
are multiple interpretations, such as whether two analyses for a
context word should be in the same reading or different readings.
We observed also cases where the grammar writer has specified desired
behaviour in comments, but the rule does not do what the grammar
writer intended. 

\begin{itemize}
\item[\textsc{s$_3$.}] \t{REMOVE Sentar IF (0 Sentar) (..) ;}
\item[] \t{SELECT PP IF (0 "estado") (..) ;}
\end{itemize}

The comments make it clear that the first rule is meant to disambiguate between \emph{sentar} and \emph{sentir}, but the rule does not mention anything about \emph{sentir}.
Even with the ambiguity class constraints, the SAT-solver only created an ambiguity where \emph{sentar} in 1st person plural is ambiguous with an anonymous 1st person plural reading.
This does not reflect the reality, where the target is only ambiguous with certain verbs, and in certain conjugated forms.

The second case is potentially more dangerous. 
The word form $estado_{W}$ 
can be either a noun ($estado_{L}$, `state'), or the past participle of the verb $estar_{L}$. 
The condition, however, addresses the lemma of the noun, $estado_L$, whereas the lemma of the PP is $estar_{L}$.
This means that, in theory, there can be a case where the condition to select the PP is already removed. As for now, the lexicon does not contain other ambiguities with the word form $estado_{W}$, but we could conceive of a scenario where someone adds e.g. a proper noun $Estado_{L}$ to the lexicon. Then, if some rule removes the lemma $estado_{L}$, the rule to select PP will not be able to trigger.


Another question is whether this level of detail is necessary. 
After all, the grammar will be used to disambiguate real life texts, where neither \emph{sentar} nor \emph{estado} are likely to have any other ambiguities.
In fact, we are planning to change how we handle the lexical forms; with those changes, it will become clear whether the imprecision will result in potential errors, given the current lexicon. 





\begin{table}[h]
\centering
%\begin{tabular}{|p{3.15cm}|p{2.55cm}|p{2.55cm}|p{2.7cm}|p{2.1cm}|}
\begin{tabular}{|p{3.05cm}|p{2.55cm}|p{2.55cm}|p{2.7cm}|p{2.1cm}|}

\hline
              & 1 clitic + 
                lemmas from 
                 grammar & 2 clitics 
                           + lemmas from 
                              grammar & 1 clitic +
                                          all readings 
                                          from grammar    & all readings from grammar \\ \hline
\# readings   & 5851 (4263~+~1588)
                       & 9494
                       (7906~+~1588) & 6657 (4263~+~2394) & 2394  \\ \hline
\#~conflicts  & 214    & 214         & 22                 & 22 \\
\small{(internal + 
 interaction)} & (211~+~3) & (211~+~3) & (19~+~3)        &  (19~+~3)  \\ \hline

\clock{} all 
       rules (approx.) & \~{}4h 30min     & \~{}9h 30min    & \~{}7h 45min &  \~{}2h 30min \\ \hline


\end{tabular}
\caption{Results for Finnish (1185 rules).}
\label{table:resFin}
\end{table}

\subsection{Finnish} 

The results for the Finnish grammar are shown separately, in Table~\ref{table:resFin}. We encountered a number of difficulties and used a few shortcuts, which we did not need for the other grammars---most importantly, not using the ambiguity class constraints. Due to these complications, the results are not directly comparable, but we include Finnish in any case, 
to give an idea how our method scales up: both to more rules, and more complex rules.

\paragraph{Challenges with Finnish} The first challenge is the morphological complexity of Finnish.
There are more than 20,000 readings, when all possible clitic combinations are included.
After weeding out the most uncommon combinations, we ended up with sets of 4000--8000 readings.

The second challenge comes from the larger size of the grammar. Whereas the Spanish and Dutch had only tens of word forms or lemmas, the Finnish grammar specifies around 900 of them.
Due to both of these factors, the procedure described in Section~\ref{sec:realistic_readings} would have exploded the number of readings, so we simply took the lemmas and word forms, and added them as single readings. 
In cases where they were combined with another tag in the grammar, we took that reading directly: for instance, we included both \emph{aika} and ``\emph{aika} n'' from the rule \texttt{SELECT "aika" + N},
but nothing from the rule \texttt{SELECT Pron + Sg}. This method gave us 1588 additional readings.

Finally, we were not able to create ambiguity class constraints---expanding the Finnish morphological lexicon results in 100s of gigabytes of word forms, which is simply too big for our method to work. 
For future development, we will see if it is possible to manipulate the finite automata directly to get hold of the ambiguities, instead of relying on the output in text.


\paragraph{Results}
The results are shown in Table~\ref{table:resFin}.
In the first column, we included only possessive suffixes. In the second column, we included question clitics as well.
Both of these readings include the 1588 lemmas and word forms from the grammar.
In the third column, we included all the tag combinations specified in the grammar, and in the fourth, we took only those, ignoring the morphological lexicon.

The first two variants reported a high number of internal conflicts. 
These are almost all due to nonexisting tags. The grammar was written in 1995, and updated by \newcite{pirinen2015}; such a high number of internal conflicts indicates that possibly something has gone wrong in the conversion, or in our expansion of the morphological lexicon.
As for accuracy, adding the question clitics did not change anything: they were already included in some of the 1588 sets with word forms or lemmas, and that was enough for the SAT-solver to find models with question clitics.
We left the result in the table just to demonstrate the change in the running time.

%Example reading that adds question clitic to our arsenal: \texttt{SELECT "joka" + Pron + Q}

The second two variants are playing with the full set of readings from the grammar. For both of these, the number of reported conflicts was only 22.
Given the preliminary nature of the results, we did not do a full analysis of all the 214 reported conflicts.
Out of the 22, we found 17 of them as true conflicts,
 but 5 seemed to be caused by our handling of rules with \verb!*!: all of these 5 rules contain a LINK and multiple \verb!*!s. On a positive note, our naive handling of the \verb!*! seems to cover the simplest cases.





Some examples of true positives are shown in the following.

%SELECT ("oma" gen) IF (NOT 1 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0 LINK *-1 "olla"|"voida"|"saattaa") (0 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0C nom)

\begin{itemize}
\item[\textsc{f$_1$}.]\t{"oma" SELECT Gen IF (..) (0C Nom) ;}

\item[] \t{SELECT Adv IF (NOT 0 PP) (..) ;}
\end{itemize}

Both of these are internal conflicts, which may not be trivial to see.
The first rule requires the target to be genitive and unambiguously nominative; however, these two tags cannot combine in the same reading.
As for the second rule, the definition of \texttt{PP} includes \texttt{adv} among others---with the sets expanded, this rule becomes \texttt{SELECT adv IF (NOT 0 pp|adv|adp|po|pr) (...)}.

The following two examples are interaction conflicts:

\begin{itemize}
\item[\textsc{f$_2$}.]\begin{verbatim}
REMOVE A (0 Der) ; 
REMOVE N (0 Der) ; 
REMOVE A/N (0 Der) ; 
\end{verbatim}
\end{itemize}

This is the same pattern we have already seen before, but with a set of rules as the reason for conflict.
The first two rules together remove the target of the third, leaving no way for there to be adjective or noun.

\begin{itemize}
\item[\textsc{f$_3$.}]\begin{verbatim}
SELECT .. IF (-1 Comma/N/Pron/Q) ;
SELECT .. IF (-2 ..) (-1 Comma) ;
\end{verbatim}
\end{itemize}

The rules above have been simplified to show only the relevant part.
The conflict lies in the fact that \texttt{Comma} is a subset of \texttt{Comma/N/Pron/Q}:
there is no way to trigger the second rule without placing a comma in position -1, and thereby triggering the first rule.




\subsection{Performance} 
The running time of the grammars ranges from seconds to hours. 
Note that the times in the Finnish table are not entirely comparable with each other: we were forced to run the tests in smaller batches, and it is possible that there are different overheads, unrelated to the size of the SAT-problem, from testing 50 or 500 rules at a time. 
Despite the inaccuracies, we can see that increasing the number of readings 
and adding the ambiguity class constraints
slow the program down significantly.
%In order to understand the effect of each feature more precisely, we should run our tests on grammars that differ on only one of the variables. 

However, many of the use cases do not require running the whole
grammar. Testing the interaction between 5--10 rules takes just
seconds in all languages, if the ambiguity class constraints are not included. 
A downside in the ambiguity classes is that generating them takes a long time, 
and while the overhead may be acceptable when checking the full grammar,
it is hardly so when analysing just a handful of rules. 
We are working on an option to store and reuse the ambiguity class constraints.

\section{Conclusions and future work}

We set out to design and implement an automatic analysis of constraint grammars that can find problematic rules and rule combinations, without the need for a corpus.
Our evaluation indicates that the tool indeed finds non-trivial conflicts and dead rules
from actual grammars. 

We did not have a volunteer to test the tool in
the process of grammar writing, so we cannot conclude whether the
constructed examples are useful for getting new insights on the rules.
In any case, there are still a number of features to improve and add.
Future work can be divided in roughly three categories: 
\begin{inparaenum}
\item[(a)] general improvement of the tool
\item[(b)] thorough evaluation in contact with user base, and
\item[(c)] integration as a part of CG development framework.
\end{inparaenum}

\subsection{General improvement}

The following is a list of improvements to the current features; we have found 

\paragraph{Combining morphological and lexical tags}

Our solution to hardcode the tag combinations in the readings is
feasible for simple morphology, but it can cause problems with more
complex morphology.
Currently, if we add one new lemma to the set of readings, we need to
create as many new variables as there are inflectional forms
for that lemma. %easily hundreds for languages with rich morphology. 

We are currently working on adding the concepts of lemmas and word
forms directly to the representation of the possible readings.
A possible solution would be to make each tag a variable, and ask the
question ``can this reading be a noun? singular? 
conditional?'' separately for each tag. Then we could lift the
restriction of tag combinations into the SAT side: make SAT-clauses
that prohibit a comparative to go with a verb, or conditional with a noun.
Alternatively, we can still hardcode the set of morphological readings, and 
only use SAT-clauses to restrict which lexical form can go with which morphological analysis.

\paragraph{Heuristic checks for common issues} 
As mentioned earlier, some grammar design choices  
are common sources of misinterpretation.
Many of these issues concern the case where the conditions include 
the target cohort---does \t{SELECT foo IF (0 bar)} mean that ``foo'' and ``bar'' 
should be in the same reading or in different readings? 
Lemmas and word forms are another source of confusion, which is easy to check automatically against the lexicon. Ideally, these checks should be included in a special ``paranoid mode'', to not clutter the analysis.


\paragraph{Full expressivity of CG-3}
As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add},  \textsc{append} and
\textsc{substitute} rules, as well as dependency structure. 
This also means finding different kinds of conflicts.
In order to implement rules that may add new readings, or new tags to
existing readings, we need to modify our approach in the SAT-encoding.
Even if the lexicon gives all readings that exist in the lexicon, the
user might give a nonexistent reading, or in the case of {\sc map}, a
syntactic tag, which is (by definition) not in the lexicon. We may need to move
to a more scalable solution.

\subsection{Evaluation with user base}

Our next step is to evaluate our tools together with actual grammar writers,
in comparison with a corpus-based method or machine learning. 
Below, we envision some properties that might be interesting; however, we would be interested in getting feedback from actual grammarians and adding features based on what is needed.

\paragraph{Reformatting a rule}

Another possible feature is to suggest reformattings for a rule. Recall
Figure~\ref{fig:infrules} from the introduction; in the case on the right, the
original rule was written by the original author, and another
grammarian thought that the latter form is nicer to read. Doing the
reverse operation could also be possible. If a rule with long
disjunctions conflicts, it may be useful to split it into smaller
conditions, and eliminate one at a time, in order to find the
reason(s) for the conflict.


\paragraph{Suggesting alternative orders} 
On a speculative note, it could be interesting to identify pairs for a potential ``feeding order'' that is missed in the grammar. Say we have the following rule sequence:

\begin{verbatim}
REMOVE:r1 x IF (-1C y)
SELECT:s2 y IF (...)
\end{verbatim}

If $s2$ appears before $r1$, if makes way for $r1$ to act later on the same round. 
However, if the rules are ordered as shown, and $y$ is not unambiguous from the beginning, then $r1$ has to wait for the next round to be applied.



Lager \cite{lager01transformation} observed that the rule sequence learned by the $\mu$TBL system did not remove further ambiguities after its first run, and concluded that the sequence was ``optimal''. 
It would be interesting to recreate the goals in \cite{lager01transformation} and \cite{bick2013tuning}, to see if this semantic analysis of rule dependencies could lead also to better ordering within a grammar.

Of course, it remains to be seen if any of these improvements would make a difference in performance; VISL CG-3 is already very fast, when the grammars are run multiple times.

\paragraph{More cohesive grammar} \cite{voutilainen2004} state that the around 200 rules are probably enough to resolve 50--75 \% of ambiguities in the corpus used in the development. 
This figure is nice and encouraging. It's very much thanks to Zipf's law: we can add rules that target the most frequent \emph{tokens}, thus disambiguating a high number of word forms. However, this does not guarantee a coherent whole of rules. 
While the coverage information is easy to obtain from a corpus, our SAT-based tool could potentially compliment the process, and guide grammar writers towards a wide coverage of different linguistic phenomena.

\section{Integration as a part of CG development environment}

In order to attract the attention of the CG community, it would be desirable to incorporate the tools as a part of existing CG software. 
Currently, the described software consists of just under 3000 lines of Haskell code, including both the CG engine and the grammar analysis tool.
The grammar used for parsing the original CG files is written in BNFC \todo{cite}, and it is missing many constructs in CG-3. 
Given these factors, the preferred option would be a full reimplementation and integration as a part of VISL CG-3, or any other CG development framework. We believe this would make the tools easier to use, more cohesive and syncronised with the main CG engine, and likely much faster. Of course, it is up to the community and the developers to decide if these tools are of interest.







%%%%%%%
% Too verbose beginning

% In the previous chapter, we have seen the SAT encoding of CG used to
% create a CG engine.
% We evaluated our engine against the state-of-the-art VISL CG-3, using
% the same grammar and same gold standard corpus.
% Unsurprisingly, we got worse results when using the SAT-based parallel
% implementation on grammars that were written for an imperative and
% sequential CG engine. 
% Given that most real grammars out there are written in such way, 
% using SAT in the CG engine offers little practical use.


% In the implementation described in the previous paragraph, we analysed
% some real input sentences, and generated clauses of the rules that
% applied to those particular sentences. If there is a word that is
% analysed as n or v, it can only match rules that target those analyses
% (and is surrounded by appropriate context).

% Now, we operate on \emph{symbolic sentences}. We start from a
% situation where each word in the sentence can have any analysis: this
% means that every rule potentially applies to every word. This
% combination of rule application starts narrowing down the potential
% sentence.
% The rules are interpreted as more abstract and declarative:
% \texttt{REMOVE verb IF -1 det} does not just check if a particular
% word is verb, it prohibits a combination of determiner followed by
% verb \emph{anywhere}. The restriction can show in various ways, 
% %if another rule requires the 3rd word of the sentence to be determiner, then the 4th may not be a verb. If a rule requires
% and must be in sync with other restrictions.

% If it turns out that there is no symbolic sentence that can satisfy a
% number of rules, this means that there is a conflict among the
% rules. In the following chapter, we will give examples of such
% conflicts and describe how to detect them.


