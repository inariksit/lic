\chapter{Introduction to this thesis}


\emph{I wish a wish}.


\noindent How do you know that the first \emph{wish} is a verb and the second one is a noun? A computer can approach this from many angles.

\begin{enumerate}
\item We can gather example sentences and annotate them manually: ``this \emph{wish} is a verb, that one is a noun''. Based on these examples, the computer will learn to guess the part of speech for new instances of \emph{wish} with a certain probability, depending on what other words appear in the sentence.

\item We can write a generative grammar to describe our language. 
A parser is only able to generate a well-formed tree for the sentence when the first \emph{wish} is a verb and the second is a noun; alternative hypotheses are cut down, because they would not fit in the structure.


\item We can look up all morphological analyses, separately for each word.
Given that we do not know anything about the context in the lookup phase, we end up with analyses $wish_N$ and $wish_V$ for both of the instances.
In contrast to approach 2, which starts with no information, we start with too much information.  
Now, in the spirit of approach 1, we can look at the context words for help. A learning-based method may encounter plenty of examples where \emph{wish} appearing after \emph{a} is a noun, and learn to assign a high probability to that hypothesis. 
But we have more powerful tools, namely, grammatical categories and abstraction.
Instead of learning facts about strings \emph{a} and \emph{wish} , or \emph{my} and \emph{house} , we can formulate a \emph{constraint rule} : ``If you find something that can be a verb, but it is preceded by a determiner, throw that guess away''. 
Add some hundreds of these rules, and they expose the structure hidden behind the clutter.
\end{enumerate}

The method number 3 is known as \emph{Constraint Grammar} (CG, Karlsson 1995).  
It is robust, fast, easy to get started, even small grammars are useful.  


\section{Scope of this thesis}
This thesis has two main contributions in the field of CG.

\paragraph{Formalisation of CG}

Based on Lager and Nivre, we did more stuff. With SAT!

Can we have a notion of ``the language generated by the grammar'' with Constraint Grammar?  
We can use the symbolic sentence approach: assuming that the initial input can be anything, then the rules would actually describe what is allowed in the language.

\paragraph{Analysis and quality control of CG}

The same factors that make it easy to write, also make it easy to write poorly.  
Even with the most careful grammar writers, managing big grammars is demanding.

We use the newly found stuff (with SAT!) to analyse CGs.
With the rules modelled in logic, we can track their effects and find out if they contradict each other.




\section{Structure of this thesis}

Chapter 2 presents a general introduction to both CG and SAT, aimed for a reader who is unfamiliar with the topics.
Chapter 3 discusses previous logical representations of CG, and describes our SAT-encoding in detail, complete with an appendix.
Chapter 4 presents the method of grammar analysis using our SAT-based implementation, along with evaluation on three different grammars.
Chapter 5 concludes the thesis.
